# Deep Learning Theory
æ•´ç†äº†ä¸€äº›æ·±åº¦å­¦ä¹ çš„ç†è®ºç›¸å…³å†…å®¹ï¼ŒæŒç»­æ›´æ–°ã€‚
## Overview
1. [Recent advances in deep learning theory](https://arxiv.org/pdf/2012.10931.pdf)
æ€»ç»“äº†ç›®å‰æ·±åº¦å­¦ä¹ ç†è®ºç ”ç©¶çš„å…­ä¸ªæ–¹å‘çš„ä¸€äº›ç»“æœï¼Œæ¦‚è¿°å‹ï¼Œæ²¡åšæ·±å…¥æ¢è®¨(2021)ã€‚

    + 1.1 complexity and capacity-basedapproaches for analyzing the generalizability of deep learning; 

    + 1.2 stochastic differential equations andtheir dynamic systems for modelling stochastic gradient descent and its variants, which characterizethe optimization and generalization of deep learning, partially inspired by Bayesian inference; 

    + 1.3 thegeometrical structures of the loss landscape that drives the trajectories of the dynamic systems;

    + 1.4 theroles of over-parameterization of deep neural networks from both positive and negative perspectives; 

    + 1.5 theoretical foundations of several special structures in network architectures; 

    + 1.6 the increasinglyintensive concerns in ethics and security and their relationships with generalizability
2. ğŸ”¥ [On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence](https://arxiv.org/abs/2207.04630)(2022.7.é©¬æ¯…ã€æ²ˆå‘æ´‹ã€æ›¹é¢–)"ä»»ä½•ä¸€ä¸ªæ™ºèƒ½ç³»ç»Ÿï¼Œä½œä¸ºä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„è‡ªä¸»é—­ç¯ï¼šä¿¡æ¯ã€æ§åˆ¶ã€å¯¹ç­–ã€ä¼˜åŒ–ã€ç¥ç»ç½‘ç»œï¼Œç´§å¯†ç»“åˆï¼Œç¼ºä¸€ä¸å¯ã€‚æ€»è€Œè¨€ä¹‹ï¼Œäººå·¥æ™ºèƒ½çš„ç ”ç©¶ä»ç°åœ¨å¼€å§‹ï¼Œåº”è¯¥èƒ½å¤Ÿä¹Ÿå¿…é¡»ä¸ç§‘å­¦ã€æ•°å­¦ã€å’Œè®¡ç®—ç´§å¯†ç»“åˆã€‚ä»æœ€æ ¹æœ¬ã€æœ€åŸºç¡€çš„ç¬¬ä¸€æ€§åŸç†ï¼ˆç®€çº¦ã€è‡ªæ´½ï¼‰å‡ºå‘ï¼ŒæŠŠåŸºäºç»éªŒçš„å½’çº³æ–¹æ³•ä¸åŸºäºåŸºç¡€åŸç†çš„æ¼”ç»æ–¹æ³•ä¸¥æ ¼åœ°ã€ç³»ç»Ÿåœ°ç»“åˆèµ·æ¥å‘å±•ã€‚ç†è®ºä¸å®è·µç´§å¯†ç»“åˆã€ç›¸è¾…ç›¸æˆã€å…±åŒæ¨è¿›æˆ‘ä»¬å¯¹æ™ºèƒ½çš„ç†è§£ã€‚" çŸ¥ä¹è§£è¯»å¯çœ‹ï¼šhttps://zhuanlan.zhihu.com/p/543041107 ã€‚
3. [Machine Learning in Physics and Geometry](https://arxiv.org/abs/2303.12626)(2023.3)æ€»ç»“äº†mlåœ¨å‡ ä½•å’Œç†è®ºç‰©ç†ä¸Šçš„ä¸€äº›è¿›å±•ï¼Œä¸»è¦æ¶‰åŠåˆ°pca,t-sne,k-means,nn,svmï¼Œæ•°æ®æ‹“æ‰‘åˆ†ææŠ€æœ¯å†…å®¹ï¼Œç†è®ºç‰©ç†åŒ…å«äº†Polytopesï¼ŒAmoebaeï¼ŒQuiversï¼ŒBrane Websã€‚

## Course
2. [Theory of Deep Learning](https://www.ideal.northwestern.edu/special-quarters/fall-2020/)TTIC,è¥¿åŒ—å¤§å­¦ç­‰ç»„ç»‡çš„ä¸€ç³»åˆ—è¯¾ç¨‹å’Œè®²åº§ï¼ŒåŸºç¡€è¯¾ç¨‹æ¶‰åŠDLçš„åŸºç¡€(ç¬¦å·åŒ–ï¼Œç®€åŒ–åçš„æ•°å­¦é—®é¢˜å’Œç»“è®º)ï¼Œä¿¡æ¯è®ºå’Œå­¦ä¹ ï¼Œç»Ÿè®¡å’Œè®¡ç®—ï¼Œä¿¡æ¯è®ºï¼Œç»Ÿè®¡å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ (2020)ã€‚

2+. [2021 Deep learning theory lecture note](https://mjt.cs.illinois.edu/dlt/ )
    + 2.1 é€¼è¿‘ï¼Œä¼˜åŒ–ï¼Œé€šç”¨æ€§ï¼Œä¸‰æ–¹é¢åšäº†æ€»ç»“ï¼Œæ ¸å¿ƒå†…å®¹ç½‘é¡µå¯è§ï¼Œæ¯”è¾ƒå‹å¥½ã€‚

3. [MathsDL-spring19](https://joanbruna.github.io/MathsDL-spring19/),MathDLç³»åˆ—ï¼Œ18,19,20å¹´å‡æœ‰ã€‚

    + 3.1  Geometry of Data

        + Euclidean Geometry: transportation metrics, CNNs , scattering.
        + Non-Euclidean Geometry: Graph Neural Networks.
        + Unsupervised Learning under Geometric Priors (Implicit vs explicit models, microcanonical, transportation metrics).
        + Applications and Open Problems: adversarial examples, graph inference, inverse problems.

    + 3.2 Geometry of Optimization and Generalization

        + Stochastic Optimization (Robbins & Munro, Convergence of SGD)
        + Stochastic Differential Equations (Fokker-Plank, Gradient Flow, Langevin + + Dynamics, links with SGD; open problems)
        Dynamics of Neural Network Optimization (Mean Field Models using Optimal Transport, Kernel Methods)
        + Landscape of Deep Learning Optimization (Tensor/Matrix factorization, Deep Nets; open problems).
        + Generalization in Deep Learning.

    + 3.3  Open qustions on Reinforcement Learning
4. [IFT 6169: Theoretical principles for deep learning](http://mitliagkas.github.io/ift6085-dl-theory-class/)(2022 Winter),å¤§å¤šå†…å®¹è¾ƒä¸ºåŸºç¡€ï¼Œä¼ ç»Ÿã€‚
    + 4.1 æ‹Ÿå®šè¯¾é¢˜
        + Generalization: theoretical analysis and practical bounds
        + Information theory and its applications in ML (information bottleneck, lower bounds etc.)
        +  Generative models beyond the pretty pictures: a tool for traversing the data manifold, projections, completion, substitutions etc.
        + Taming adversarial objectives: Wasserstein GANs, regularization approaches and + controlling the dynamics
        + The expressive power of deep networks (deep information propagation, mean-field analysis of random networks etc.)
5. [æ·±åº¦å­¦ä¹ å‡ ä½•è¯¾ç¨‹](https://geometricdeeplearning.com/lectures/)(2022, Michael Bronstein)å†…å®¹æ¯”è¾ƒé«˜çº§.
    + 5.1 2022 å¹´çš„ GDL100 å…±åŒ…å« 12 èŠ‚å¸¸è§„è¯¾ç¨‹ã€3 èŠ‚è¾…å¯¼è¯¾ç¨‹å’Œ 5 æ¬¡ä¸“é¢˜ç ”è®¨ã€‚12 èŠ‚å¸¸è§„è¯¾ç¨‹ä¸»è¦ä»‹ç»äº†å‡ ä½•æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µçŸ¥è¯†ï¼ŒåŒ…æ‹¬é«˜ç»´å­¦ä¹ ã€å‡ ä½•å…ˆéªŒçŸ¥è¯†ã€å›¾ä¸é›†åˆã€ç½‘æ ¼ï¼ˆgridï¼‰ã€ç¾¤ã€æµ‹åœ°çº¿ï¼ˆgeodesicï¼‰ã€æµå½¢ï¼ˆmanifoldï¼‰ã€è§„èŒƒï¼ˆgaugeï¼‰ç­‰ã€‚3 èŠ‚è¾…å¯¼è¯¾ä¸»è¦é¢å‘è¡¨è¾¾å‹å›¾ç¥ç»ç½‘ç»œã€ç¾¤ç­‰å˜ç¥ç»ç½‘ç»œå’Œå‡ ä½•å›¾ç¥ç»ç½‘ç»œã€‚

    + 5 æ¬¡ä¸“é¢˜ç ”è®¨çš„è¯é¢˜åˆ†åˆ«æ˜¯ï¼š
        1. ä»å¤šç²’å­åŠ¨åŠ›å­¦å’Œæ¢¯åº¦æµçš„è§’åº¦åˆ†æç¥ç»ç½‘ç»œï¼›
        2. è¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„ GNN å­å›¾ï¼›
        3. æœºå™¨å­¦ä¹ ä¸­çš„ç­‰å˜æ€§ï¼›
        4. ç¥ç» sheaf æ‰©æ•£ï¼šä»æ‹“æ‰‘çš„è§’åº¦åˆ†æ GNN ä¸­çš„å¼‚è´¨æ€§å’Œè¿‡åº¦å¹³æ»‘ï¼›
        5. ä½¿ç”¨ AlphaFold è¿›è¡Œé«˜åº¦å‡†ç¡®çš„è›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€‚

6. [Advanced Topics in Machine Learning and Game Theory](https://feifang.info/advanced-topics-in-machine-learning-and-game-theory-fall-2022/)æ¸¸æˆï¼Œå¼ºåŒ–æ–¹é¢çš„è¯¾ç¨‹ï¼Œ2022ã€‚
## Architecture
5. [Partial Differential Equations is All You Need for Generating Neural Architectures -- A Theory for Physical Artificial Intelligence Systems](https://arxiv.org/abs/2103.08313) å°†ç»Ÿè®¡ç‰©ç†çš„ååº”æ‰©æ•£æ–¹ç¨‹ï¼Œé‡å­åŠ›å­¦ä¸­çš„è–›å®šè°”æ–¹ç¨‹ï¼Œå‚è½´å…‰å­¦ä¸­çš„äº¥å§†éœå…¹æ–¹ç¨‹ç»Ÿä¸€æ•´åˆåˆ°ç¥ç»ç½‘ç»œåå¾®åˆ†æ–¹ç¨‹ä¸­(NPDE)ï¼Œåˆ©ç”¨æœ‰é™å…ƒæ–¹æ³•æ‰¾åˆ°æ•°å€¼è§£ï¼Œä»ç¦»æ•£è¿‡ç¨‹ä¸­ï¼Œæ„é€ äº†å¤šå±‚æ„ŸçŸ¥ï¼Œå·ç§¯ç½‘ç»œï¼Œå’Œå¾ªç¯ç½‘ç»œï¼Œå¹¶æä¾›äº†ä¼˜åŒ–æ–¹æ³•L-BFGSç­‰ï¼Œä¸»è¦æ˜¯å»ºç«‹äº†ç»å…¸ç‰©ç†æ¨¡å‹å’Œç»å…¸ç¥ç»ç½‘ç»œçš„è”ç³»(2021)ã€‚

## Approximation
6. NN Approximation Theory
    + 6.0 [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)NNé€¼è¿‘ä»è¿™é‡Œå¼€å§‹(1991)
    + 6.1 [Cybenkoâ€™s Theorem and the capabilityof a neural networkas function approximator](https://www.mathematik.uni-wuerzburg.de/fileadmin/10040900/2019/Seminar__Artificial_Neural_Network__24_9__.pdf)ä¸€äºŒç»´shallowç¥ç»ç½‘ç»œçš„å¯è§†åŒ–è¯æ˜(2019)
    + 6.2 [Depth Separation for Neural Networks](https://arxiv.org/abs/1702.08489) ä¸‰å±‚ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›æ¯”ä¸¤å±‚æœ‰ä¼˜è¶Šæ€§çš„ç®€åŒ–è¯æ˜ (2017)
    + 6.3 [èµµæ‹“ç­‰ä¸‰ç¯‡](https://www.zhihu.com/question/347654789/answer/1480974642)(2019)
    + 6.4 [Neural Networks with Small Weights and Depth-Separation Barriers](https://arxiv.org/abs/2006.00625) Gal Vardiç­‰è¯æ˜äº†å¯¹æŸäº›ç±»å‹çš„ç¥ç»ç½‘ç»œ, ç”¨kå±‚çš„å¤šé¡¹å¼è§„æ¨¡ç½‘ç»œéœ€è¦ä»»æ„weight, ä½†ç”¨3k+3å±‚çš„å¤šé¡¹å¼è§„æ¨¡ç½‘ç»œåªéœ€è¦å¤šé¡¹å¼å¤§å°çš„ weight(2020)ã€‚
    + 6.5 [Universality of Deep Convolutional Neural Networks](https://arxiv.org/pdf/1805.10769.pdf)å·ç§¯ç½‘ç»œçš„é€šç”¨é€¼è¿‘èƒ½åŠ›ï¼ŒåŠå…¶æ ¸å¿ƒè¦ç´ ï¼ŒDing-Xuan Zhou(2018)ï¼Œ20å¹´å‘è¡¨ã€‚
    + 6.6 [Deep Learning and Approximation theory 2023](https://nadavdym.github.io/lecture_notes/DeepNotes.pdf ) å…³äºDLå’Œé€¼è¿‘ç†è®ºçš„notesï¼Œ62é¡µï¼Œæ¯”è¾ƒæ¸…æ™°å®Œå–„ã€‚ 

## Optimization
7. SGD
    + 7.1 [nesterov-accelerated-gradient](https://paperswithcode.com/method/nesterov-accelerated-gradient)

8. [offconvex](http://www.offconvex.org/)å‡ ä¸ªå­¦æœ¯å·¥ä½œè€…ç»´æŠ¤çš„AIåšå®¢ã€‚
    + 8.1 [beyondNTK](http://www.offconvex.org/2021/03/25/beyondNTK/) ä»€ä¹ˆæ—¶å€™NNå¼ºäºNTKï¼Ÿ
    + 8.2 [instahide](http://www.offconvex.org/2020/11/11/instahide/) å¦‚ä½•åœ¨ä¸æ³„éœ²æ•°æ®çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨¡å‹ï¼Ÿ
    + 8.3 [implicit regularization in DL explained by norms?](http://www.offconvex.org/2020/11/27/reg_dl_not_norm/)

9. [Adam](https://arxiv.org/abs/1412.6980)
    + 9.1 [deep-learning-dynamics-paper-list](https://github.com/zeke-xie/deep-learning-dynamics-paper-list)å…³äºDLä¼˜åŒ–åŠ¨åŠ›å­¦æ–¹é¢ç ”ç©¶çš„èµ„æ–™æ”¶é›†ã€‚
    + 9.2 [Adai](https://github.com/zeke-xie/adaptive-inertia-adai) Adamçš„ä¼˜åŒ–ç‰ˆæœ¬Adaiï¼ŒAdamé€ƒç¦»éç‚¹å¾ˆå¿«ï¼Œä½†æ˜¯ä¸èƒ½åƒSGDä¸€æ ·æ“…é•¿æ‰¾åˆ°flat minimaã€‚ä½œè€…è®¾è®¡ä¸€ç±»æ–°çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨Adaiç»“åˆSGDå’ŒAdamçš„ä¼˜ç‚¹ã€‚Adaié€ƒç¦»éç‚¹é€Ÿåº¦æ¥è¿‘Adam,å¯»æ‰¾flat minimaèƒ½æ¥è¿‘SGDã€‚å…¶çŸ¥ä¹ä»‹ç»å¯çœ‹[Adai-zhihu]((https://www.zhihu.com/question/323747423/answer/2576604040))
10. [ Smooth momentum: improving lipschitzness in gradient descent ](https://link.springer.com/article/10.1007/s10489-022-04207-7) æå‡ºäº†å¹³æ»‘åŠ¨é‡ï¼Œä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ï¼Œå®ƒæ”¹å–„äº†åœ¨â€é™¡å³­å¢™å£â€ä¸Šçš„è¡Œä¸ºã€‚ä½œè€…å¯¹æ‰€æå‡ºçš„ä¼˜åŒ–å™¨çš„ç‰¹æ€§è¿›è¡Œäº†æ•°å­¦åˆ†æï¼Œå¹¶è¯æ˜äº†å¹³æ»‘åŠ¨é‡å±•ç°å‡ºæ”¹è¿›çš„åˆ©æ™®å¸ŒèŒ¨ç‰¹æ€§å’Œæ”¶æ•›æ€§ï¼Œè¿™å…è®¸åœ¨æ¢¯åº¦ä¸‹é™ä¸­ç¨³å®šä¸”æ›´å¿«çš„æ”¶æ•›ã€‚ (2023)


## Geometry
9. Optima transmission
    + 9.1 [æ·±åº¦å­¦ä¹ çš„å‡ ä½•å­¦è§£é‡Š](http://www.engineering.org.cn/ch/10.1016/j.eng.2019.09.010)(2020)

## Book
10. [Theory of Deep Learning(draft)](https://www.cs.princeton.edu/courses/archive/fall19/cos597B/lecnotes/bookdraft.pdf)Rong Ge ç­‰(2019)ã€‚

11. [Spectral Learning on Matrices and Tensors](https://arxiv.org/pdf/2004.07984.pdf)Majid Janzaminç­‰(2020)

19. [Deep Learning Architectures A Mathematical Approach](https://www.springer.com/gp/book/9783030367206)(2020),ä½ å¯ä»¥libgenè·å–ï¼Œå†…å®¹å¦‚å…¶åå­—,å¤§æ¦‚åŒ…å«ï¼šå·¥ä¸šé—®é¢˜ï¼ŒDLåŸºç¡€(æ¿€æ´»ï¼Œç»“æ„ï¼Œä¼˜åŒ–ç­‰),å‡½æ•°é€¼è¿‘ï¼Œä¸‡æœ‰é€¼è¿‘ï¼ŒRELUç­‰é€¼è¿‘æ–°ç ”ç©¶ï¼Œå‡½æ•°è¡¨ç¤ºï¼Œä»¥åŠä¸¤å¤§æ–¹å‘ï¼Œä¿¡æ¯è§’åº¦ï¼Œå‡ ä½•è§’åº¦ç­‰ç›¸å…³çŸ¥è¯†ï¼Œå®é™…åœºæ™¯ä¸­çš„å·ç§¯ï¼Œæ± åŒ–ï¼Œå¾ªç¯ï¼Œç”Ÿæˆï¼Œéšæœºç½‘ç»œç­‰å…·ä½“å®ç”¨å†…å®¹çš„æ•°å­¦åŒ–ï¼Œå¦å¤–é™„å½•é›†åˆè®ºï¼Œæµ‹åº¦è®ºï¼Œæ¦‚ç‡è®ºï¼Œæ³›å‡½ï¼Œå®åˆ†æç­‰åŸºç¡€çŸ¥è¯†ã€‚
20. [The Principles of Deep Learning Theory](https://arxiv.org/pdf/2106.10165.pdf)(2021)Daniel A. Roberts and Sho Yaida(mit)ï¼ŒBeginning from a first-principles component-level picture of networksï¼Œæœ¬ä¹¦è§£é‡Šäº†å¦‚ä½•é€šè¿‡æ±‚è§£å±‚åˆ°å±‚è¿­ä»£æ–¹ç¨‹å’Œéçº¿æ€§å­¦ä¹ åŠ¨åŠ›å­¦æ¥ç¡®å®šè®­ç»ƒç½‘ç»œè¾“å‡ºçš„å‡†ç¡®æè¿°ã€‚ä¸€ä¸ªä¸»è¦çš„ç»“æœæ˜¯ç½‘ç»œçš„é¢„æµ‹æ˜¯ç”±è¿‘é«˜æ–¯åˆ†å¸ƒæè¿°çš„ï¼Œç½‘ç»œçš„æ·±åº¦ä¸å®½åº¦çš„çºµæ¨ªæ¯”æ§åˆ¶ç€ä¸æ— é™å®½åº¦é«˜æ–¯æè¿°çš„åå·®ã€‚æœ¬ä¹¦è§£é‡Šäº†è¿™äº›æœ‰æ•ˆæ·±åº¦ç½‘ç»œå¦‚ä½•ä»è®­ç»ƒä¸­å­¦ä¹ éå¹³å‡¡çš„è¡¨ç¤ºï¼Œå¹¶æ›´å¹¿æ³›åœ°åˆ†æéçº¿æ€§æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ æœºåˆ¶ã€‚ä»è¿‘å†…æ ¸æ–¹æ³•çš„è§’åº¦æ¥çœ‹ï¼Œå‘ç°è¿™äº›æ¨¡å‹çš„é¢„æµ‹å¯¹åº•å±‚å­¦ä¹ ç®—æ³•çš„ä¾èµ–å¯ä»¥ç”¨ä¸€ç§ç®€å•è€Œé€šç”¨çš„æ–¹å¼æ¥è¡¨è¾¾ã€‚ä¸ºäº†è·å¾—è¿™äº›ç»“æœï¼Œä½œè€…å¼€å‘äº†è¡¨ç¤ºç»„æµï¼ˆRG æµï¼‰çš„æ¦‚å¿µæ¥è¡¨å¾ä¿¡å·é€šè¿‡ç½‘ç»œçš„ä¼ æ’­ã€‚é€šè¿‡å°†ç½‘ç»œè°ƒæ•´åˆ°ä¸´ç•ŒçŠ¶æ€ï¼Œä»–ä»¬ä¸ºæ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜æä¾›äº†ä¸€ä¸ªå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ä½œè€…è¿›ä¸€æ­¥è§£é‡Šäº† RG æµå¦‚ä½•å¯¼è‡´è¿‘ä¹æ™®éçš„è¡Œä¸ºï¼Œä»è€Œå¯ä»¥å°†ç”±ä¸åŒæ¿€æ´»å‡½æ•°æ„å»ºçš„ç½‘ç»œåšç±»åˆ«åˆ’åˆ†ã€‚Altogether, they show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networksã€‚åˆ©ç”¨ä¿¡æ¯ç†è®ºï¼Œä½œè€…ä¼°è®¡äº†æ¨¡å‹æ€§èƒ½æœ€å¥½çš„æœ€ä½³æ·±å®½æ¯”ï¼Œå¹¶è¯æ˜äº†æ®‹å·®è¿æ¥èƒ½å°†æ·±åº¦æ¨å‘ä»»æ„æ·±åº¦ã€‚åˆ©ç”¨ä»¥ä¸Šç†è®ºå·¥å…·ï¼Œå°±å¯ä»¥æ›´åŠ ç»†è‡´çš„ç ”ç©¶æ¶æ„çš„å½’çº³åå·®ï¼Œè¶…å‚æ•°ï¼Œä¼˜åŒ–ã€‚[åŸä½œè€…çš„è§†é¢‘è¯´æ˜](https://www.youtube.com/watch?v=wXZKoHEzASg)(2021.12.1)
21. [Physics-based Deep Learning](https://arxiv.org/pdf/2109.05237.pdf)(2021)N. Thuerey, P. Holl,etc.[github resources](https://github.com/thunil/Physics-Based-Deep-Learning)æ·±åº¦å­¦ä¹ ä¸ç‰©ç†å­¦çš„è”ç³»ã€‚æ¯”å¦‚åŸºäºç‰©ç†çš„æŸå¤±å‡½æ•°ï¼Œå¯å¾®æµä½“æ¨¡æ‹Ÿï¼Œé€†é—®é¢˜çš„æ±‚è§£ï¼ŒNavier-Stokesæ–¹ç¨‹çš„å‰å‘æ¨¡æ‹Ÿï¼ŒControlling Burgersâ€™ Equationå’Œå¼ºåŒ–å­¦ä¹ çš„å…³ç³»ç­‰ã€‚
22. [Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/abs/2104.13478)(Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar VeliÄkoviÄ‡,2021),è§ä¸Šé¢è¯¾ç¨‹5:æ·±åº¦å­¦ä¹ å‡ ä½•è¯¾ç¨‹.
23. [dynamical systems and ml 2, 2020](https://www.stat.berkeley.edu/~mmahoney/talks/dynamical_systems_and_ml_2.pdf)çœ‹èµ·æ¥å†™å¾—å¾ˆå¥½ï¼Œæœ‰Connection between ResNets and Dynamical Systemsã€‚
24. [Dynamical Systems and Machine Learning 2020, pku](https://www.math.pku.edu.cn/amel/docs/20200719122925684287.pdf)ã€‚

## Session
21. [Foundations of Deep Learning](https://simons.berkeley.edu/programs/dl2019)(2019)ï¼Œè¥¿è’™ç ”ç©¶ä¸­å¿ƒä¼šè®®ã€‚
22. [Deep Learning Theory 4](https://icml.cc/virtual/2021/session/12048)(2021, ICML)Claire Monteleoniä¸»æŒ...,æ·±åº¦å­¦ä¹ ç†è®ºä¼šè®®4ï¼ŒåŒ…å«è®ºæ–‡å’Œè§†é¢‘ã€‚
23. [Deep Learning Theory 5 ](https://icml.cc/virtual/2021/session/12057)(2021,ICML)MaYiä¸»æŒ...ï¼Œæ·±åº¦å­¦ä¹ ç†è®ºä¼šè®®5ï¼ŒåŒ…å«è®ºæ–‡å’Œè§†é¢‘ã€‚
24. [DeLTA 2023 : 4th International Conference on Deep Learning Theory and Applications](http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=170637&copyownerid=45217) ä¼šè®®åŒ…å«RNN,CNN,DHN,GAN,AE,EV,Dimensionality Reductionç­‰åŸºæœ¬æ¨¡å‹å†…å®¹ï¼Œå…·ä½“å•¥å†…å®¹æœªçŸ¥ï¼ŒMLçš„åŸºæœ¬å†…å®¹ï¼Œå¼ºåŒ–ï¼Œåº¦é‡ï¼Œæ ¸ï¼Œå›¾è¡¨ç¤ºï¼Œèšç±»ï¼Œåˆ†ç±»ï¼Œå›å½’ç­‰ï¼Œè¿˜æœ‰å¤§æ•°æ®ï¼Œå›¾åƒçš„å…·ä½“åº”ç”¨æ–¹å‘ï¼Œè¯­è¨€ç†è§£æ–¹å‘ã€‚çœ‹èµ·æ¥éƒ½æ˜¯æ¯”è¾ƒåŸºç¡€å†…å®¹ã€‚
25. [å’ŒåŠ¨åŠ›ç³»ç»Ÿçš„ä¼šè®® 2023](https://machinelearning-dynamic.github.io/)ã€‚[é‡Œé¢æ”¶åˆ°çš„æ–‡ç« åˆ—è¡¨](https://machinelearning-dynamic.github.io/schedule.html )

## generalization
1. [Robust Learning with Jacobian Regularization](https://arxiv.org/abs/1908.02729)(2019)Judy Hoffman..., 
2. [Predicting Generalization using GANs](http://www.offconvex.org/2022/06/06/PGDL/)(2022.6),ç”¨GANæ¥è¯„ä¼°æ³›åŒ–æ€§.
3. [Implicit Regularization in Tensor Factorization: Can Tensor Rank Shed Light on Generalization in Deep Learning?](http://www.offconvex.org/2021/07/08/imp-reg-tf/)(2021.7)Tensor Rank èƒ½å¦æ­ç¤ºæ·±åº¦å­¦ä¹ ä¸­çš„æ³›åŒ–ï¼Ÿ
4. [å¦‚ä½•é€šè¿‡Meta Learningå®ç°åŸŸæ³›åŒ–Domain Generalization](https://mp.weixin.qq.com/s/o1liWf9B4_LntBeyV2bVOg)(2022.4),[Domain Generalization CVPR2022](https://mp.weixin.qq.com/s/HkjHEqs8d85VPdgpaHEzPQ)åšæ–‡å‚è€ƒ.
5. [Generalization-Causality](https://github.com/yfzhang114/Generalization-Causality) ä¸€åšå£«å…³äºdomain generalizationç­‰å·¥ä½œçš„å®æ—¶æ±‡æ€»ã€‚
6. [Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Networks](http://www.offconvex.org/2022/07/15/imp-reg-htf-cnn/)(Noam Razin  â€¢  Jul 15, 2022)

    + 6.1 Across three different neural network types (equivalent to matrix, tensor, and hierarchical tensor factorizations), we have an architecture-dependant notion of rank that is implicitly lowered. Moreover, the underlying mechanism for this implicit regularization is identical in all cases. This leads us to believe that implicit regularization towards low rank may be a general phenomenon. If true, finding notions of rank lowered for different architectures can facilitate an understanding of generalization in deep learning.

    + 6.2 Our findings imply that the tendency of modern convolutional networks towards locality may largely be due to implicit regularization, and not an inherent limitation of expressive power as often believed. More broadly, they showcase that deep learning architectures considered suboptimal for certain tasks can be greatly improved through a right choice of explicit regularization. Theoretical understanding of implicit regularization may be key to discovering such regularizers.


## Others
4. [Theoretical issues in deep networks](https://www.pnas.org/content/117/48/30039) è¡¨æ˜æŒ‡æ•°å‹æŸå¤±å‡½æ•°ä¸­å­˜åœ¨éšå¼çš„æ­£åˆ™åŒ–ï¼Œå…¶ä¼˜åŒ–çš„ç»“æœå’Œä¸€èˆ¬æŸå¤±å‡½æ•°ä¼˜åŒ–ç»“æœä¸€è‡´ï¼Œä¼˜åŒ–æ”¶æ•›ç»“æœå’Œæ¢¯åº¦æµçš„è¿¹æœ‰å…³ï¼Œç›®å‰è¿˜ä¸èƒ½è¯æ˜å“ªä¸ªç»“æœæœ€ä¼˜(2020)ã€‚
12. [The Dawning of a New Erain Applied Mathematics](https://www.ams.org/journals/notices/202104/rnoti-p565.pdf)Weinan Eå…³äºåœ¨DLçš„æ–°å¤„å¢ƒä¸‹ç»“åˆå†å²çš„å·¥ä½œèŒƒå¼ç»™å‡ºçš„æŒ‡å¯¼æ€§æ€»ç»“(2021)ã€‚
13. [Mathematics of deep learning from Newton Institute](https://www.newton.ac.uk/event/mdl)ã€‚
14. [DEEP NETWORKS FROM THE PRINCIPLE OF RATE REDUCTION](https://openreview.net/forum?id=G70Z8ds32C9)ï¼Œç™½ç›’ç¥ç»ç½‘ç»œã€‚
15. [redunet_paper](https://github.com/ryanchankh/redunet_paper)ç™½ç›’ç¥ç»ç½‘ç»œä»£ç ã€‚
16. [Theory of Deep Convolutional Neural Networks:Downsampling](https://www.cityu.edu.hk/rcms/pdf/XDZhou/dxZhou2020b.pdf)ä¸‹é‡‡æ ·çš„æ•°å­¦åˆ†æDing-Xuan Zhou(2020)
17. [Theory of deep convolutional neural networks II: Spherical analysis](https://arxiv.org/abs/2007.14285)è¿˜æœ‰IIIï¼šradial functions é€¼è¿‘ï¼Œ(2020)ã€‚ä¸è¿‡è¿™äº›å·¥ä½œåˆ°åº•å¦‚ä½•ï¼Œåªæ˜¯ç”¨æ•°å­¦è½¬æ¢äº†ä¸€ä¸‹ï¼Œç†è®ºä¸Šæ²¡åšè¿‡å¤šè´¡çŒ®ï¼Œæˆ–è€…å’Œå®é™…ç»“åˆæ²¡éš¾ä¹ˆç´§å¯†ï¼Œè¿˜ä¸å¾—è€ŒçŸ¥ã€‚
18. [The Modern Mathematics of Deep Learning](https://arxiv.org/abs/2105.04026)(2021)ä¸»è¦æ˜¯deep laerningçš„æ•°å­¦åˆ†ææè¿°ï¼Œæ¶‰åŠçš„é—®é¢˜åŒ…æ‹¬ï¼šè¶…å‚æ•°ç½‘ç»œçš„é€šç”¨èƒ½åŠ›ï¼Œæ·±åº¦åœ¨æ·±åº¦æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ·±åº¦å­¦ä¹ å¯¹ç»´åº¦ç¾éš¾çš„å…‹æœï¼Œä¼˜åŒ–åœ¨éå‡¸ä¼˜åŒ–é—®é¢˜çš„æˆåŠŸï¼Œå­¦ä¹ çš„è¡¨ç¤ºç‰¹å¾çš„æ•°å­¦åˆ†æï¼Œä¸ºä½•æ·±åº¦æ¨¡å‹åœ¨ç‰©ç†é—®é¢˜ä¸Šæœ‰è¶…å¸¸è¡¨ç°ï¼Œæ¨¡å‹æ¶æ„ä¸­çš„å“ªäº›å› ç´ ä»¥ä½•ç§æ–¹å¼å½±å“ä¸åŒä»»åŠ¡çš„å­¦ä¹ ä¸­çš„ä¸åŒæ–¹é¢ã€‚
19. [Topos and Stacks of Deep Neural Networks](https://arxiv.org/abs/2106.14587)(2021)æ¯ä¸€ä¸ªå·²çŸ¥çš„æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)å¯¹åº”äºä¸€ä¸ªå…¸å‹çš„ Grothendieck çš„ topos ä¸­çš„ä¸€ä¸ªå¯¹è±¡; å®ƒçš„å­¦ä¹ åŠ¨æ€å¯¹åº”äºè¿™ä¸ª topos ä¸­çš„ä¸€ä¸ªæ€å°„æµã€‚å±‚ä¸­çš„ä¸å˜æ€§ç»“æ„(å¦‚ CNNs æˆ– LSTMs)ä¸Giraud's stacksç›¸å¯¹åº”ã€‚è¿™ç§ä¸å˜æ€§è¢«è®¤ä¸ºæ˜¯æ³›åŒ–æ€§è´¨çš„åŸå› ï¼Œå³ä»çº¦æŸæ¡ä»¶ä¸‹çš„å­¦ä¹ æ•°æ®è¿›è¡Œæ¨æ–­ã€‚çº¤ç»´ä»£è¡¨å‰è¯­ä¹‰ç±»åˆ«(Culioliï¼ŒThom) ï¼Œå…¶ä¸Šäººå·¥è¯­è¨€çš„å®šä¹‰ï¼Œå†…éƒ¨é€»è¾‘ï¼Œç›´è§‰ä¸»ä¹‰ï¼Œç»å…¸æˆ–çº¿æ€§(Girard)ã€‚ç½‘ç»œçš„è¯­ä¹‰åŠŸèƒ½æ˜¯ç”¨è¿™ç§è¯­è¨€è¡¨è¾¾ç†è®ºçš„èƒ½åŠ›ï¼Œç”¨äºå›ç­”è¾“å…¥æ•°æ®è¾“å‡ºä¸­çš„é—®é¢˜ã€‚è¯­ä¹‰ä¿¡æ¯çš„é‡å’Œç©ºé—´çš„å®šä¹‰ä¸é¦™å†œç†µçš„åŒæºè§£é‡Šç›¸ç±»ä¼¼ã€‚ä»–ä»¬æ¨å¹¿äº† Carnap å’Œ Bar-Hillel (1952)æ‰€å‘ç°çš„åº¦é‡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸Šè¿°è¯­ä¹‰ç»“æ„è¢«åˆ†ç±»ä¸ºå‡ ä½•çº¤ç»´å¯¹è±¡åœ¨ä¸€ä¸ªå°é—­çš„Quillenæ¨¡å‹èŒƒç•´ï¼Œç„¶åä»–ä»¬å¼•èµ·åŒæ—¶å±€éƒ¨ä¸å˜çš„ dnn å’Œä»–ä»¬çš„è¯­ä¹‰åŠŸèƒ½ã€‚Intentional type theories (Martin-Loef)ç»„ç»‡è¿™äº›å¯¹è±¡å’Œå®ƒä»¬ä¹‹é—´çš„çº¤ç»´åŒ–ã€‚ä¿¡æ¯å†…å®¹å’Œäº¤æ¢ç”± Grothendieck's derivatorsåˆ†æã€‚
20. [Visualizing the Emergence of Intermediate Visual Patterns in DNNs](https://arxiv.org/abs/2111.03505)(2021,NIPS)æ–‡ç« è®¾è®¡äº†ä¸€ç§ç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„å¯è§†åŒ–æ–¹æ³•ï¼Œä½¿å¾—èƒ½
ï¼ˆ1ï¼‰æ›´ç›´è§‚åœ°åˆ†æç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶ä¸”å±•ç¤ºä¸­å±‚ç‰¹å¾è¡¨è¾¾èƒ½åŠ›çš„æ—¶ç©ºæ¶Œç°ï¼›
ï¼ˆ2ï¼‰é‡åŒ–ç¥ç»ç½‘ç»œä¸­å±‚çŸ¥è¯†ç‚¹ï¼Œä»è€Œå®šé‡åœ°åˆ†æç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„è´¨é‡ï¼›
ï¼ˆ3ï¼‰ä¸ºä¸€äº›æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚å¯¹æŠ—æ”»å‡»ã€çŸ¥è¯†è’¸é¦ï¼‰æä¾›æ–°è§è§£ã€‚
21.  [ç¥ç»ç½‘ç»œçš„åšå¼ˆäº¤äº’è§£é‡Šæ€§](https://zhuanlan.zhihu.com/p/264871522/)(çŸ¥ä¹)ã€‚ä¸Šäº¤å¤§å¼ æ‹³çŸ³å›¢é˜Ÿç ”ç©¶è®ºæ–‡æ•´ç†è€Œå¾—ï¼Œä½œä¸ºåšå¼ˆäº¤äº’è§£é‡Šæ€§çš„ä½“ç³»æ¡†æ¶ï¼ˆä¸æ€ä¹ˆç¨³å›ºï¼‰ã€‚
22.  [Advancing mathematics by guiding human intuition with AI](https://www.nature.com/articles/s41586-021-04086-x)(2021,nature)æœºå™¨å­¦ä¹ å’Œæ•°å­¦å®¶å·¥ä½œçš„ä¸€ä¸ªæœ‰æœºç»“åˆï¼Œä¸»è¦åˆ©ç”¨æœºå™¨å­¦ä¹ åˆ†æä¼—å¤šç‰¹å¾å’Œç›®æ ‡å˜é‡çš„ä¸»è¦ç›¸å…³å› å­ï¼ŒåŠ å¼ºæ•°å­¦å®¶çš„ç›´è§‰ï¼Œè¯¥è®ºæ–‡å¾—åˆ°äº†ä¸¤ä¸ªæ¼‚äº®çš„å®šç†ï¼Œä¸€ä¸ªæ‹“æ‰‘ï¼Œä¸€ä¸ªè¡¨ç¤ºè®ºã€‚å¯å‚è€ƒ[å›ç­”](https://www.zhihu.com/question/503185412/answer/2256015652)ã€‚
23. ğŸ”¥[A New Perspective of Entropy](https://math3ma.institute/wp-content/uploads/2022/02/bradley_spring22.pdf)(2022) é€šè¿‡è±å¸ƒå°¼å…¹å¾®åˆ†æ³•åˆ™(Leibniz rule)å°†ä¿¡æ¯ç†µ,æŠ½è±¡ä»£æ•°,æ‹“
æ‰‘å­¦è”ç³»èµ·æ¥ã€‚è¯¥æ–‡ç« æ˜¯ä¸€ä¸ªé›¶åŸºç¡€å¯é˜…è¯»çš„ç»¼è¿°,å…·ä½“å‚è€ƒ[Entropy as a Topological Operad Derivation ](https://www.mdpi.com/1099-4300/23/9/1195)(2021.7,Tai-Danae Bradley.)
24. [minerva](https://storage.googleapis.com/minerva-paper/minerva_paper.pdf)(2022)googleæå‡ºçš„è§£é¢˜æ¨¡å‹,åœ¨å…¬å…±é«˜ç­‰æ•°å­¦ç­‰è€ƒè¯•ä¸­æ¯”äººç±»å¹³å‡åˆ†é«˜.[æµ‹è¯•åœ°å€](https://minerva-demo.
github.io/#category=Algebra&index=1).
25. ğŸ”¥[An automatic theorem proving project](https://gowers.wordpress.com/2022/04/28/announcing-an-automatic-theorem-proving-project/#more-6531)è²å°”å…¹è·å¾—è€…æ•°å­¦å®¶é«˜å°”æ–¯å…³äº
è‡ªåŠ¨è¯æ˜æ•°å­¦å®šç†çš„é¡¹ç›®è¿›å±•[How can it be feasible to find proofs?](https://drive.google.com/file/d/1-FFa6nMVg18m1zPtoAQrFalwpx2YaGK4/view)(2022, W.T. Gowers).
26. [GRAND: Graph Neural Diffusion ](https://papertalk.org/papertalks/32188)(2021)è¯¥ç½‘ç«™åŒ…å«äº†ä¸€äº›ç›¸ä¼¼è®ºæ–‡èµ„æ–™,[é¡¹ç›®åœ°å€graph-neural-pde](https://github.com/twitter-research
/graph-neural-pde),å…¶ä¼˜åŒ–ç‰ˆæœ¬[GRAND++](https://openreview.net/forum?id=EMxu-dzvJk).(2022).æœ‰åšæ–‡ä»‹ç»[å›¾ç¥ç»ç½‘ç»œçš„å›°å¢ƒï¼Œç”¨å¾®åˆ†å‡ ä½•å’Œä»£æ•°æ‹“æ‰‘è§£å†³](https://mp.weixin.qq.com/s/CFNvgn6vaYcI36QJNa3_dw)ä»…ä¾›å‚
è€ƒ.
27. [Weinan Ãˆ-A Mathematical Perspective on Machine Learning](https://opade.digital/)(2022.icm),room1æœ€åä¸€æ’,é„‚ç»´å—åœ¨icmçš„æ¼”è®²è§†é¢‘.
28.  [contrastive learning](https://zhuanlan.zhihu.com/p/524733769)è¯æ˜åŒ…æ‹¬InfoNCEåœ¨å†…çš„ä¸€å¤§ç±»å¯¹æ¯”å­¦ä¹ ç›®æ ‡å‡½æ•°ï¼Œç­‰ä»·äºä¸€ä¸ªæœ‰ä¸¤ç±»å˜é‡ï¼ˆæˆ–è€…è¯´ä¸¤ç±»ç©å®¶ï¼‰å‚ä¸çš„äº¤æ›¿ä¼˜åŒ–ï¼ˆæˆ–è€…è¯´æ¸¸æˆï¼‰è¿‡ç¨‹.
29.  [å¯è§£é‡Šæ€§ï¼šBatch Normalizationæœªå¿…å®¢è§‚åœ°ååº”æŸå¤±å‡½æ•°ä¿¡æ¯](https://zhuanlan.zhihu.com/p/523627298)2022,å¼ æ‹³çŸ³ç­‰.
30. [Homotopy Theoretic and Categorical Models of Neural Information Networks](https://arxiv.org/abs/2006.15136)è¯¥å·¥ä½œç¬¬ä¸€ä½œè€…ä¿„ç½—æ–¯æ•°å­¦å®¶Yuri Maninï¼Œ2020å·¥ä½œï¼Œ2022å¹´8æœˆarxivæœ‰æ›´æ–°ã€‚[ncatlabæœ‰è®¨è®º](https://nforum.ncatlab.org/discussion/13133/understanding-preprint-topos-and-stacks-of-deep-neural-networks/)ã€‚[åšæ–‡è®²è§£](http://www.neverendingbooks.org/deep-learning-and-toposes)ã€‚
31. [Deep learning via dynamical systems: An approximation perspective](https://ems.press/journals/jems/articles/5404458)åŠ¨åŠ›ç³»ç»Ÿé€¼è¿‘ã€‚[è®ºæ–‡è§](https://cpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/d/11132/files/2021/01/main-jems.pdf)ã€‚
32. [ç¾¤è®ºè§’åº¦](https://www.youtube.com/playlist?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd)ç¾¤è®ºè§’åº¦å»ç†è§£çš„ä¸€ç³»åˆ—è§†é¢‘ï¼Œç¾¤è®ºè§†è§’ï¼Œ2014å¹´å‡ºç°è¿‡ï¼Œè§†é¢‘ç³»ç»Ÿè®²è§£ï¼Œ2022å¹´ã€‚
33. [Constructions in combinatorics via neural networks](https://arxiv.org/abs/2104.14516)ä½œè€…Adam Zsolt Wagneré€šè¿‡ç¥ç»ç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ æ„å»ºäº†ä¸€ç³»åˆ—åä¾‹ï¼Œæ¨ç¿»äº†å‡ ä¸ªç»„åˆå­¦çš„çŒœæƒ³ï¼Œ2021å¹´ã€‚
34. [ä¸åŠ¨ç‚¹2023](https://arxiv.org/pdf/2303.12814.pdf)è¿™ç¯‡æ–‡ç« ä¸»è¦ç ”ç©¶ä»»æ„æ·±åº¦çš„ä¸€ç»´ç¥ç»ç½‘ç»œä¸­çš„å›ºå®šç‚¹é—®é¢˜ã€‚é¦–å…ˆï¼Œæ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä»£æ•°æ‹“æ‰‘ç†è®ºæ¥ç ”ç©¶ç¥ç»ç½‘ç»œä¸­çš„å›ºå®šç‚¹ã€‚ç„¶åï¼Œæ–‡ç« è¯æ˜äº†ä»»æ„æ·±åº¦çš„ä¸€ç»´ç¥ç»ç½‘ç»œåœ¨æŸäº›æƒ…å†µä¸‹ä¸€å®šå­˜åœ¨å›ºå®šç‚¹ã€‚ç‰¹åˆ«åœ°ï¼Œå½“ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°ä¸ºsigmoidå‡½æ•°æ—¶ï¼Œç½‘ç»œåœ¨å­˜åœ¨è¶³å¤Ÿå¤šçš„ç¥ç»å…ƒæ—¶å¿…å®šå­˜åœ¨å›ºå®šç‚¹ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è¯æ˜äº†æŸäº›ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œä»»æ„æ·±åº¦çš„ä¸€ç»´ç¥ç»ç½‘ç»œä¸­çš„å›ºå®šç‚¹æ˜¯ç¨ å¯†çš„ï¼Œå³åœ¨ç½‘ç»œå‚æ•°çš„ç©ºé—´ä¸­ï¼Œå­˜åœ¨æ— é™å¤šçš„å‚æ•°å¯ä»¥ä½¿å¾—ç½‘ç»œçš„è¾“å‡ºä¸ºå›ºå®šå€¼ã€‚è¿™é¡¹ç ”ç©¶å…·æœ‰æ·±å…¥æ¢ç©¶ç¥ç»ç½‘ç»œç†è®ºçš„æ„ä¹‰ï¼Œæœ‰åŠ©äºæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œåœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°å’Œæ€§è´¨ã€‚*åé¢å†…å®¹æ˜¯ç›´æ¥è¾“å…¥é¢˜ç›®ï¼Œchatgptæ€»ç»“çš„ã€‚*
35. [Overparameterized ReLU Neural Networks Learn the Simplest Model: Neural Isometry and Phase Transitions](https://arxiv.org/pdf/2209.15265.pdf)2023,è¿™æ˜¯ä¸€ç¯‡å…³äºæ·±åº¦å­¦ä¹ ç†è®ºçš„è®ºæ–‡ï¼Œç ”ç©¶äº†è¿‡å‚æ•°åŒ–çš„ReLUç¥ç»ç½‘ç»œçš„è®­ç»ƒå’Œæ³›åŒ–æ€§èƒ½ï¼Œä»¥åŠå…¶å­¦ä¹ çš„æ¨¡å‹å¤æ‚åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿ç¥ç»ç½‘ç»œçš„å‚æ•°æ•°é‡æé«˜ï¼Œå®ƒä»¬ä¹Ÿèƒ½å¤Ÿå­¦ä¹ å‡ºéå¸¸ç®€å•çš„æ¨¡å‹ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡æ™ºæ…§ç›¸çŸ›ç›¾ã€‚æœ¬æ–‡é‡‡ç”¨äº†å‡¸ä¼˜åŒ–å’Œç¨€ç–æ¢å¤çš„è§†è§’ï¼Œæå‡ºäº†ç¥ç»åŒæ„å’Œç›¸å˜çš„æ¦‚å¿µï¼Œæ¥è§£é‡Šç¥ç»ç½‘ç»œå­¦ä¹ ç®€å•æ¨¡å‹çš„åŸå› ã€‚
    æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

    1. å¯¹äºä¸¤å±‚ReLUç½‘ç»œï¼Œç ”ç©¶äº†å…¶è®­ç»ƒå’Œæ³›åŒ–æ€§èƒ½ï¼Œå¹¶è¯æ˜åªæœ‰å­¦ä¹ ç®€å•æ¨¡å‹çš„å‚æ•°æ˜¯æœ‰æ„ä¹‰çš„ã€‚
    2. æå‡ºäº†ç¥ç»åŒæ„çš„æ¦‚å¿µï¼Œå³è¾“å…¥ç©ºé—´å’Œè¾“å‡ºç©ºé—´ä¹‹é—´çš„ç­‰è·å˜æ¢ï¼Œè¿™æœ‰åŠ©äºè§£é‡Šç¥ç»ç½‘ç»œå­¦ä¹ ç®€å•æ¨¡å‹çš„åŸå› ã€‚
    3. æå‡ºäº†ç›¸å˜çš„æ¦‚å¿µï¼Œå¹¶è¯æ˜åœ¨éšæœºç”Ÿæˆçš„æ•°æ®ä¸Šï¼Œç¥ç»ç½‘ç»œçš„æ¢å¤æ€§èƒ½å­˜åœ¨ç›¸å˜ç°è±¡ã€‚å½“æ ·æœ¬æ•°é‡ä¸ç»´åº¦ä¹‹æ¯”è¶…è¿‡ä¸€ä¸ªæ•°å€¼é˜ˆå€¼æ—¶ï¼Œæ¢å¤æˆåŠŸçš„æ¦‚ç‡å¾ˆé«˜ï¼›å¦åˆ™ï¼Œå¤±è´¥çš„æ¦‚ç‡å¾ˆé«˜ã€‚
    4. ç ”ç©¶äº†å…·æœ‰è·³è·ƒè¿æ¥æˆ–å½’ä¸€åŒ–å±‚çš„ReLUç½‘ç»œï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„ç­‰è·æ¡ä»¶ï¼Œä»¥ç¡®ä¿å‡†ç¡®æ¢å¤ç§æ¤çš„ç¥ç»å…ƒã€‚

    æ€»ä¹‹ï¼Œæœ¬æ–‡é€šè¿‡ç†è®ºåˆ†æå’Œæ•°å€¼å®éªŒï¼Œè¯æ˜äº†è¿‡å‚æ•°åŒ–çš„ReLUç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ æœ€ç®€å•çš„æ¨¡å‹ï¼Œè¿™ä¸ºæ·±åº¦å­¦ä¹ çš„ç†è®ºç ”ç©¶å’Œå®é™…åº”ç”¨æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚(é—®chatgptè®ºæ–‡é¢˜ç›®ç»™çš„å›å¤)
36. [FAST COMPUTATION OF PERMUTATION EQUIVARIANT LAYERS WITH THE PARTITION ALGEBRA](https://arxiv.org/pdf/2303.06208.pdf) 2023.1, è¯¥è®ºæ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§å¿«é€Ÿè®¡ç®—ç½®æ¢ç­‰å˜å±‚çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨äº†åˆ†åŒºä»£æ•°ã€‚
37. [Omnigrok: Grokking Beyond Algorithmic Data](https://arxiv.org/abs/2210.01117)2022.10. Grokking æ˜¯ç®—æ³•æ•°æ®é›†çš„ä¸€ç§ä¸å¯»å¸¸ç°è±¡ï¼Œåœ¨è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®åå¾ˆä¹…å°±ä¼šå‘ç”Ÿæ³›åŒ–ï¼Œè¿™ç§ç°è±¡ä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡åˆ†æç¥ç»ç½‘ç»œçš„æŸå¤±æƒ…å†µæ¥ç†è§£ grokkingï¼Œå°†è®­ç»ƒå’Œæµ‹è¯•æŸå¤±ä¹‹é—´çš„ä¸åŒ¹é…ç¡®å®šä¸º grokking çš„åŸå› ã€‚æˆ‘ä»¬å°†æ­¤ç§°ä¸ºâ€œLU æœºåˆ¶â€ï¼Œå› ä¸ºè®­ç»ƒå’Œæµ‹è¯•æŸå¤±ï¼ˆé’ˆå¯¹æ¨¡å‹æƒé‡èŒƒæ•°ï¼‰é€šå¸¸åˆ†åˆ«ç±»ä¼¼äºâ€œLâ€å’Œâ€œUâ€ã€‚è¿™ç§ç®€å•çš„æœºåˆ¶å¯ä»¥å¾ˆå¥½åœ°è§£é‡Š grokking çš„è®¸å¤šæ–¹é¢ï¼šæ•°æ®å¤§å°ä¾èµ–æ€§ã€æƒé‡è¡°å‡ä¾èµ–æ€§ã€è¡¨ç¤ºçš„å‡ºç°ç­‰ã€‚åœ¨ç›´è§‚å›¾ç‰‡çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ¶‰åŠå›¾åƒã€è¯­è¨€å’Œåˆ†å­çš„ä»»åŠ¡ä¸Šè¯±å¯¼ grokkingã€‚åœ¨ç›¸åçš„æ–¹å‘ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ¶ˆé™¤ç®—æ³•æ•°æ®é›†çš„ grokkingã€‚æˆ‘ä»¬å°†ç®—æ³•æ•°æ®é›†çš„ grokking çš„æˆå‰§æ€§å½’å› äºè¡¨å¾å­¦ä¹ ã€‚
38. [Grokking modular arithmetic](https://arxiv.org/abs/2301.02679)2023.1 æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Œå®ƒå¯ä»¥å­¦ä¹ æ¨¡å—åŒ–ç®—æœ¯ä»»åŠ¡å¹¶è¡¨ç°å‡ºæ³›åŒ–çš„çªç„¶è·³è·ƒï¼Œç§°ä¸ºâ€œgrokkingâ€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºï¼ˆiï¼‰å®Œå…¨è¿æ¥çš„ä¸¤å±‚ç½‘ç»œï¼Œåœ¨æ²¡æœ‰ä»»ä½•æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹ï¼Œåœ¨æ™®é€šæ¢¯åº¦ä¸‹é™ä¸‹ä½¿ç”¨ MSE æŸå¤±å‡½æ•°è¡¨ç°å‡ºå¯¹å„ç§æ¨¡å—åŒ–ç®—æœ¯ä»»åŠ¡çš„ grokkingï¼› (ii) è¯æ®è¡¨æ˜ grokking æ¨¡å—åŒ–ç®—æ³•å¯¹åº”äºå­¦ä¹ å…¶ç»“æ„ç”±ä»»åŠ¡ç¡®å®šçš„ç‰¹å®šç‰¹å¾å›¾ï¼› (iii) æƒé‡çš„è§£æè¡¨è¾¾å¼â€”â€”ä»¥åŠç‰¹å¾å›¾â€”â€”è§£å†³ä¸€å¤§ç±»æ¨¡å—åŒ–ç®—æœ¯ä»»åŠ¡ï¼› (iv) è¯æ˜è¿™äº›ç‰¹å¾å›¾ä¹Ÿå¯ä»¥é€šè¿‡æ™®é€šæ¢¯åº¦ä¸‹é™æ³•å’Œ AdamW æ‰¾åˆ°ï¼Œä»è€Œå»ºç«‹ç½‘ç»œå­¦ä¹ è¡¨ç¤ºçš„å®Œæ•´å¯è§£é‡Šæ€§ã€‚
39. [Progress measures for grokking via mechanistic interpretability](https://arxiv.org/abs/2301.05217)2023.2ç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§æ–¹é¢ç ”ç©¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œgrokkingä¸æ˜¯çªç„¶çš„è½¬å˜ï¼Œè€Œæ˜¯æºäºæƒé‡ä¸­ç¼–ç çš„ç»“æ„åŒ–æœºåˆ¶çš„é€æ¸æ”¾å¤§ï¼Œéšåæ˜¯è®°å¿†ç»„ä»¶çš„åˆ é™¤ã€‚
40. [awesome-deep-phenomena ](https://github.com/MinghuiChen43/awesome-deep-phenomena) 2022.5ï¼Œ ç¥ç»ç½‘ç»œè¡¨ç°å‡ºçš„å„ç§ç°è±¡çš„ç ”ç©¶gitæ•´ç†ã€‚
41. [Deep Learning for Mathematical Reasoning (DL4MATH)](https://github.com/lupantech/dl4math)DLå’Œæ•°å­¦ç›¸å…³çš„èµ„æ–™gitæ•´ç†ã€‚
42. [PCAST Working Group on Generative AI Invites Public Input](https://terrytao.wordpress.com/2023/05/13/pcast-working-group-on-generative-ai-invites-public-input/)2023.5.13. é™¶å“²è½©å°†ä¸»æŒç¾å›½æ€»ç»Ÿç§‘æŠ€é¡¾é—®å§”å‘˜ä¼šï¼ˆPCASTï¼‰æˆç«‹çš„ä¸€ä¸ªç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥ä½œç»„çš„ä¼šè®®ã€‚
43. ğŸ”¥[CRATE](https://github.com/Ma-Lab-Berkeley/CRATE) ç™½ç›’ai è®­ç»ƒè¿›å±•.çŸ¥ä¹ä¸Šæœ‰äººè®¨è®ºï¼Œ<font color="red"> è¯¥æ–‡ç« æ‹¼å‡‘æ¯”è¾ƒå¤šï¼Œä¸”è®ºæ–‡å’Œä»£ç ç›¸å·®åä¸‡å…«åƒé‡Œã€‚ </font> [çŸ¥ä¹å‚è€ƒ](https://www.zhihu.com/question/634009595)ã€‚(2023)
44. [An Ambiguity Measure for Recognizing the Unknowns in Deep Learning](https://arxiv.org/pdf/2312.06077.pdf)è‡ªåä¸ºé¦™æ¸¯ç ”ç©¶ä¸­å¿ƒçš„å­¦è€…ï¼Œç»™å‡ºäº†æ¨¡å‹å¯¹æœªçŸ¥é‡(ç±»)çš„æ¨¡ç³Šæ€§åº¦é‡æ¡†æ¶ã€‚è¯¥æ¨¡ç³Šåº¦é‡èƒ½å¤Ÿåˆ¤æ–­æœªçŸ¥é‡å’Œå·²çŸ¥æ¨¡å‹æ‰€èƒ½åˆ¤æ–­çš„èŒƒå›´çš„ç›¸å…³æ€§å¤§å°ã€‚(2023)
 ## DeepModeling
1. [DeepModeling](https://deepmodeling.com/)é„‚ç»´å—ç­‰ç»„ç»‡,ä¸€ç§æ–°çš„ç ”ç©¶èŒƒå¼,å°†DLå»ºæ¨¡æ¸—é€åˆ°ç§‘ç ”ä¸­,è¿™é‡Œä¼šå¼€æºå¾ˆå¤šå¯¹æ–°æˆ–æ—§é—®é¢˜çš„DLå»ºæ¨¡æ–¹æ¡ˆ.[å…¶githubåœ°å€](https://github.com/deepmode
ling).ç©ºäº†çœ‹æƒ…å†µè§£ææŸäº›å·¥ä½œ.
2. [deepflame](https://github.com/deepmodeling/deepflame-dev)DLå‘çš„æµä½“åŠ›å­¦åŒ…ã€‚
3. [FunSearch](https://mp.weixin.qq.com/s/PLN5easZX-0wY-lcjJRJ0Q)å¤§è¯­è¨€æ¨¡å‹å’Œè‡ªåŠ¨è¯„ä¼°å™¨çš„è”åˆè¿­ä»£ï¼Œå‘ç°äº†å¸½é›†é—®é¢˜çš„æ–°è§£å†³æ–¹æ¡ˆï¼Œåœ¨ä¸€äº›è®¾å®šä¸‹å‘ç°äº†æœ‰å²ä»¥æ¥æœ€å¤§çš„å¸½é›†ã€‚DeepMindå›¢é˜Ÿè¿˜å†³å®šå°†FunSearchåº”ç”¨äºå‘ç°æ›´æœ‰æ•ˆçš„è§£å†³â€œè£…ç®±é—®é¢˜â€ï¼ˆbin packingï¼‰çš„ç®—æ³•ã€‚(2023)
4. [Applications  of Deep Learning to Scientific Computing](https://www.research-collection.ethz.ch/handle/20.500.11850/646749 )PINN, DeepONetç­‰ç”¨æ¥è§£ç‰©ç†ç›¸å…³çš„PDEæ–¹ç¨‹çš„ä¸€ç¯‡æ¥è‡ªETHçš„åšå£«è®ºæ–‡ã€‚(2023)
5. [An ML approach to resolution of singularities](https://arxiv.org/pdf/2307.00252.pdf)ä½œè€…ä»‹ç»äº†ä¸€ç§æ–°çš„Hironakaæ¸¸æˆæ–¹æ³•ï¼Œå®ƒä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»£ç†æ¥å¯»æ‰¾å¥‡ç‚¹çš„æœ€ä¼˜åˆ†è¾¨ç‡(Resolutions)ã€‚åœ¨æŸäº›é¢†åŸŸï¼Œè®­ç»ƒæœ‰ç´ çš„æ¨¡å‹åœ¨æ‰§è¡Œå¤šé¡¹å¼åŠ æ³•çš„æ€»æ¬¡æ•°æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„é€‰æ‹©å¯å‘å¼æ–¹æ³•ï¼Œè¿™è¯æ˜äº†æœºå™¨å­¦ä¹ çš„æœ€æ–°å‘å±•æœ‰æ½œåŠ›æé«˜ç¬¦å·è®¡ç®—ä¸­ç®—æ³•çš„æ€§èƒ½ã€‚(2023)  
 
 ## æ•°å­¦å½¢å¼ä¸»ä¹‰ä¸è®¡ç®—æœº
1. [The Future of Mathematicsï¼Ÿ ](https://www.bilibili.com/video/av71583469)(2019) Kevin Buzzardå°±leançš„ä¸€åœºè®²åº§ï¼Œè¯„è®ºåŒºæœ‰å¯¹åº”è®²ä¹‰èµ„æ–™ã€‚
2. [æ•°å­¦å½¢å¼ä¸»ä¹‰çš„å…´èµ·](https://mp.weixin.qq.com/s/-XosE3LzA8wfFv-38EIfKQ)(2022.7)Kevin Buzzardæ•™æˆåœ¨2022æœ¬å±Šå›½é™…æ•°å­¦å®¶å¤§ä¼šä¸€å°æ—¶æŠ¥å‘Šæ¼”è®²ä¸­æä¾›äº†ä¸€äº›ä¿¡æ¯å’Œæ€è€ƒè§è§£ã€‚è®²è¿°äº†æ•°å­¦
å½¢å¼ä¸»ä¹‰ä¸äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œå¼€æºç¤¾åŒºçš„å…±åŒåŠªåŠ›ï¼Œç”¨è®¡ç®—æœºåšå¥¥æ•°é¢˜ã€æ£€æŸ¥æ•°å­¦è¯æ˜è¿‡ç¨‹æ˜¯å¦æœ‰è¯¯ã€ç”šè‡³è‡ªåŠ¨å‘ç°å’Œå½¢å¼åŒ–è¯æ˜æ•°å­¦å®šç†ï¼Œåœ¨ç†è®ºå’Œå®è·µä¸­åˆä¼šç¢°æ’å‡ºä»€ä¹ˆç«èŠ±ï¼Œåˆä¼šå¦‚ä½•å›¿äº...
3. [ä¸“è®¿ICM 2022å›½é™…æ•°å­¦å®¶å¤§ä¼šä¸€å°æ—¶æŠ¥å‘Šè€…Kevin Buzzardï¼šè®¡ç®—æœºå¯ä»¥æˆä¸ºæ•°å­¦å®¶å—ï¼Ÿâ€”â€”è¯‘è‡ªé‡å­æ‚å¿—](https://mp.weixin.qq.com/s/VWuRyxkl0xgZWcqRn0WJAw)æ¯”è¾ƒå¥½çš„é‡‡è®¿,å€¼å¾—çœ‹çœ‹.æ•°å­¦å®¶è®©è®¡ç®—æœºç§‘å­¦å®¶äº†è§£åˆ°æ•°å­¦å¾ˆéš¾,è¿™ä¸ªéƒ¨åˆ†,åœ¨è¢«é€æ¸ç†è§£,ä¸”è®¡ç®—æœºç³»ç»Ÿæ£€æŸ¥,å¯èƒ½ä¼šè§£å†³è¿™ä¸ªéš¾ç‚¹.è¿˜æœ‰é‚£äº›ç‚«é…·çš„é¡¹ç›®,çƒé¢å¤–ç¿»,è´¹é©¬å¤§å®šç†,éå¸¸å€¼å¾—å…³æ³¨.
4. [Deep Maths-machine learning and mathematics](https://www.youtube.com/watch?v=wbJQTtjlM_w),é‡æ–°å‘ç°Eulerå¤šé¢ä½“å…¬å¼ ï¼ˆå¯¹ä¹‹å‰å·¥ä½œçš„ç»†èŠ‚çš„æ›´è¿›ä¸€æ­¥è¯´æ˜ï¼‰,æ¶‰åŠç»„åˆä¸å˜é‡çŒœæƒ³ï¼ŒåºåŠ è±çŒœæƒ³ï¼Œç‘Ÿæ–¯é¡¿å‡ ä½•çŒœæƒ³ï¼Œæ‰­ç»“å›¾ç­‰ï¼ˆæ¶‰åŠçš„é¢å¾ˆå¤§ï¼Œä½†éƒ½æ˜¯ä¸€å¸¦è€Œè¿‡ï¼‰ã€‚(2023)  
<!-- ![image](./imgs/ex1_knot.png) -->
5. [Would it be possible to create a tool to automatically diagram papers?](https://terrytao.wordpress.com/2023/02/18would-it-be-possible-to-create-a-tool-to-automatically-diagram-papers/) Taoåœ¨[IPAM](https://www.ipam.ucla.edu/programs/workshops/machine-assisted-proofs/)ç»„ç»‡ç”¨æœºå™¨å­¦ä¹ æ¥å¸®åŠ©è¯æ˜çš„workshopsã€‚
6. [æ•°å­¦çš„å½¢å¼åŒ–ä¸AI for MathematicsåŒ—å¤§](https://mp.weixin.qq.com/s/_8h-qJ4GYf52HQrL0hl60g)æ²¡æ‰¾åˆ°å…·ä½“å†…å®¹ï¼Œå¯ä»¥å¿½ç•¥(å…·ä½“å†…å®¹å·²ç”±åŸä½œè€…åˆ†äº«äº†ï¼Œå¯ç§èŠ)ï¼Œ2023.3ã€‚
7. [To Teach Computers Math, Researchers Merge AI Approaches](https://www.quantamagazine.org/to-teach-computers-math-researchers-merge-ai-approaches-20230215/)å¤§æ¨¡å‹ï¼Œè‡ªç„¶è¯­è¨€çš„æ•°å­¦è¯æ˜èƒ½åŠ›æ¢ç´¢(2023.4)ã€‚
8. [Is deep learning a useful tool for the pure mathematician?](https://arxiv.org/abs/2304.12602)Geordie Williamson,2023.4ã€‚Claudeå†…å®¹æ€»ç»“ï¼šè¿™ç¯‡æ–‡ç« ä¸»è¦æ¢è®¨äº†æ·±åº¦å­¦ä¹ å¯¹çº¯æ•°å­¦å®¶çš„å®ç”¨æ€§ã€‚æ–‡ç« è®¤ä¸º:1. æ·±åº¦å­¦ä¹ çš„è®¸å¤šæˆå°±éƒ½ä¾èµ–äºéµå¾ªç®€å•è§„åˆ™çš„å¤§è§„æ¨¡ç»Ÿè®¡å»ºæ¨¡,è¿™ä¸æ•°å­¦å®¶å¯»æ‰¾ç®€æ´è€Œæ·±åˆ»çš„ç†è®ºä¸åŒã€‚å› æ­¤,æ·±åº¦å­¦ä¹ å¯èƒ½ä¸ä¼šç›´æ¥æ¨åŠ¨æ•°å­¦ç†è®ºçš„å‘å±•ã€‚2. ä½†æ˜¯,æ·±åº¦å­¦ä¹ å¯ä»¥äº§ç”Ÿä¸€äº›æœ‰è¶£çš„æ•°å­¦é—®é¢˜ã€‚ä¾‹å¦‚,ç†è§£ç¥ç»ç½‘ç»œä¸ºä»€ä¹ˆåœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚,æ¢ç´¢è¿™äº›ç½‘ç»œå­¦ä¹ çš„è¡¨è¾¾å½¢å¼å’Œç®—æ³•åŸºç¡€,è¿™å¯èƒ½ä¼šäº§ç”Ÿä¸€äº›æœ‰ä»·å€¼çš„æ•°å­¦ç†è®ºã€‚3. æ·±åº¦å­¦ä¹ ä¹Ÿå¯ä»¥æˆä¸ºä¸€ç§å‘ç°æ–°çš„æ•°å­¦å…³ç³»å’Œç»“æ„çš„å·¥å…·ã€‚ç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¥ç†è§£å¤æ‚ç³»ç»Ÿçš„æ‹“æ‰‘å’Œå‡ ä½•ç»“æ„ã€‚è¿™å¯ä»¥ä¸ºæ•°å­¦å®¶æä¾›ä¸€äº›æœ‰è¶£çš„æ–°æ€è·¯å’Œè§è§£ã€‚4. æ·±åº¦å­¦ä¹ ä¹Ÿè¢«ç”¨æ¥åŠ é€Ÿå®šç†è¯æ˜å’Œå…¬å¼æ¨å¯¼ç­‰ä¼ ç»Ÿçš„æ•°å­¦å·¥ä½œã€‚è¿™å¯ä»¥ä½¿æ•°å­¦ç ”ç©¶å˜å¾—æ›´åŠ æ•°æ®é©±åŠ¨å’Œè‡ªåŠ¨åŒ–ã€‚ä¸€äº›ç ”ç©¶å·²ç»å–å¾—äº†æ—©æœŸæˆåŠŸ,è¿™æ˜¯ä¸€ä¸ªå€¼å¾—æœŸå¾…çš„æ–°æ–¹å‘ã€‚5. æ€»ä½“è€Œè¨€,å°½ç®¡æ·±åº¦å­¦ä¹ å’Œæ•°å­¦ç†è®ºæœ‰ä¸åŒçš„åŠ¨æœºä¸æ–¹æ³•è®º,ä½†ä¸¤è€…ä¹‹é—´å­˜åœ¨æ½œåœ¨çš„ååŒæ€§ã€‚æ·±åº¦å­¦ä¹ å¯ä»¥æˆä¸ºæ•°å­¦å‘ç°å’Œç†è§£çš„æœ‰åŠ›å·¥å…·,æ¨åŠ¨æ›´å¹¿æ³›åœ°è·¨å­¦ç§‘åˆä½œã€‚ä½†å®ƒä¸å¤ªå¯èƒ½ç›´æ¥å½±å“æ•°å­¦åŸºç¡€ç†è®ºçš„å‘å±•ã€‚æ–‡ç« è®¤ä¸º,æ·±åº¦å­¦ä¹ å¯ä»¥æˆä¸ºæ•°å­¦å®¶çš„ä¸€ä¸ªæœ‰ç”¨å·¥å…·,ç‰¹åˆ«æ˜¯åœ¨æ¨åŠ¨æ–°å‘ç°ã€æ–°è§è§£å’Œæ–°æ–¹å‘ä¸Šã€‚ä½†å®ƒå¯èƒ½æ— æ³•ç›´æ¥æ¨åŠ¨æ›´åŠ åŸºç¡€çš„æ•°å­¦ç†è®ºå»ºè®¾ã€‚ä¸¤è€…æœ‰æ½œåœ¨çš„äº’è¡¥ä¸ååŒä½œç”¨,å€¼å¾—åŠ å¼ºè·¨é¢†åŸŸåˆä½œä¸ç†è§£ã€‚overall,æ·±åº¦å­¦ä¹ æœ‰åŠ©äºæ‰©å±•å’Œæ‹“å±•å½“å‰çš„æ•°å­¦ä½“ç³»,ä½†ä¸ä¼šé¢ è¦†å…¶åŸºæœ¬å»ºç«‹çš„ç†è®ºåŸºç¡€ã€‚
9. [Learning proofs for the classification of nilpotent semigroups](https://arxiv.org/abs/2106.03015)Carlos Simpson,2021.6ã€‚Claudeæ€»ç»“ï¼šè¿™ç¯‡æ–‡ç« æ¢ç´¢äº†ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡ŒnilpotentåŠç¾¤çš„åˆ†ç±»å®šç†è¯æ˜,å¹¶åœ¨å®éªŒä¸Šå–å¾—äº†åˆæ­¥æˆåŠŸã€‚ä½†æ˜¯å®Œå…¨è‡ªåŠ¨åŒ–å’Œä»¤äººç†è§£çš„å®šç†è¯æ˜è¿˜éœ€è¦æ›´å¤šå·¥ä½œã€‚æœºå™¨å­¦ä¹ åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­æ›´å¯èƒ½èµ·è¾…åŠ©ä½œç”¨,æˆä¸ºå‘ç°å®šç†å’Œç†è§£ä»£æ•°ç»“æ„çš„æœ‰åŠ›å·¥å…·ã€‚è¿™æ˜¯ä¸€ä¸ªæ¶‰åŠæ•°å­¦ä¸äººå·¥æ™ºèƒ½äº¤å‰çš„æœ‰è¶£ä¸»é¢˜,å€¼å¾—è¿›ä¸€æ­¥æ¢è®¨ä¸å®è·µã€‚
10. [Lean for the Curious Mathematician 2023](https://lftcm2023.github.io/)2023å¹´ä¸¾åŠçš„â€œLean for the Curious Mathematicianâ€æ´»åŠ¨ã€‚è®²åº§å°†ä»åŸºç¡€å¼€å§‹(å¦‚ä½•å®‰è£…å’Œä½¿ç”¨Lean,å½¢å¼åŒ–æ•°å­¦çš„åŸºæœ¬æœºåˆ¶,æµè§ˆproof library mathlibç­‰),ç„¶åæ„å»ºæ›´é«˜çº§çš„ä¸»é¢˜,ä»¥è¯´æ˜å½¢å¼åŒ–åœ¨ä¸åŒæ•°å­¦é¢†åŸŸçš„å·¥ä½œæ–¹å¼ã€‚å…·ä½“çš„é¢†åŸŸå–å†³äºå‚ä¸è€…çš„å…´è¶£(å¦‚å®åˆ†æã€å¤åˆ†æã€å¾®åˆ†å‡ ä½•ã€æ•°è®ºã€æ‹“æ‰‘å­¦ã€ç»„åˆæ•°å­¦ã€èŒƒç•´è®ºç­‰)ã€‚
11. [Fermat's Last Theorem for regular primes](https://arxiv.org/abs/2305.08955) Leanåœ¨è´¹é©¬å¤§å®šç†æŒ‡æ•°ä¸ºæ­£åˆ™ç´ æ•°(REGULAR PRIMES)æƒ…å½¢çš„è¯æ˜ã€‚[æ•°å­¦è¯æ˜å¯å‚è€ƒ](https://kconrad.math.uconn.edu/blurbs/gradnumthy/fltreg.pdf)(2023.5.15)ã€‚è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œä½†ç¦»å®Œæ•´ç‰ˆçš„è¯æ˜ï¼Œå·®è·è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ã€‚
12. [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models ](https://arxiv.org/abs/2306.15626)(2023)å°†å¤§è¯­è¨€æ¨¡å‹å’Œå½¢å¼åŒ–è¯æ˜ç»“åˆèµ·æ¥ï¼Œå®˜æ–¹ç›¸å…³éƒ¨ç½²[LeanCopilot](https://github.com/lean-dojo/LeanCopilot )éƒ½åœ¨åšã€‚
13. [pfr](https://github.com/teorth/pfr) ä¸€ä¸ªç”±å››ä½è‘—åæ•°å­¦å®¶ç»„æˆçš„å›¢é˜Ÿï¼ŒåŒ…æ‹¬ä¸¤ä½è²å°”å…¹å¥–å¾—ä¸»ï¼Œè¯æ˜äº†ä¸€ä¸ªè¢«æè¿°ä¸ºâ€œåŠ æ³•ç»„åˆå­¦åœ£æ¯â€ï¼ˆholy grail of additive combinatoricsï¼‰çš„çŒœæƒ³ã€‚åœ¨ä¸€ä¸ªæœˆå†…ä»–ä»¬æ¾æ•£åœ°åˆä½œï¼Œç”¨è®¡ç®—æœºè¾…åŠ©è¯æ˜éªŒè¯äº†å®ƒã€‚è¾…åŠ©è¯æ˜ï¼šè“å›¾å¯ä»¥åˆ›å»ºä¸€ä¸ªå›¾è¡¨ [dep graph document](https://teorth.github.io/pfr/blueprint/dep_graph_document.html) æè¿°è¯æ˜ä¸­æ¶‰åŠçš„å„ç§é€»è¾‘æ­¥éª¤ï¼ŒLeanæ¥å½¢å¼åŒ–è¯æ˜éªŒè¯ã€‚ ç›¸å…³æŠ¥å‘Š[å‚è€ƒ1](https://mp.weixin.qq.com/s/l3Zm2HXOrxiSkaSk0dbNDg),[å‚è€ƒ2](https://terrytao.wordpress.com/2023/12/05/a-slightly-longer-lean-4-proof-tour/)
## Discussion
1. [æ€æ ·çœ‹å¾…Ali Rahimi è·å¾— NIPS 2017 Test-of-time Awardåçš„æ¼”è®²ï¼Ÿ](https://www.zhihu.com/question/263711574)17å¹´å°±æœ‰äºº(å¼ å¿ƒæ¬£,ç‹åˆšç­‰)æŒ‡å‡ºäº†DLçš„ç¼ºé™·,å’Œè¿™ä¸ªé¢†åŸŸä¸­äººçš„ç‰¹ç‚¹,è¿‡å»5å¹´äº†,è¿˜æ˜¯é‚£æ ·.ä¸è¿‡å¦‚23 èƒ½çœ‹å‡º,metaçš„åšåº”ç”¨çš„ç”°æ¸Šæ ‹è¿˜åœ¨åšå®ˆç†è®º.
2. [æ·±åº¦å­¦ä¹ é¢†åŸŸæœ‰å“ªäº›ç“¶é¢ˆï¼Ÿ](https://www.zhihu.com/question/40577663/answer/2593884415)å¼ æ‹³çŸ³æ–°çš„åæ§½,ä»¥åŠæœ€æ–°æˆæœæ±‡é›†.
3. [ChatGPT/GPT4](https://openai.com/research/gpt-4)è™½ç„¶å’Œç†è®ºæ— å…³ï¼Œä½†å®ç”¨æ€§å¾ˆå¥½çš„ä¸€ä¸ªè¿›å±•ï¼Œç›®å‰å‡†ç¡®ç‡(!èƒ¡æ‰¯ç‡)è²Œä¼¼èƒ½è¾¾åˆ°0.8+ã€‚
4. [æ•°å€¼pdeä¸æ·±åº¦å­¦ä¹ ç»“åˆæ˜¯æœªæ¥å‘å±•æ–¹å‘å—ï¼Ÿ](https://www.zhihu.com/question/523893840)
5. [å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¶Œç°ç°è±¡æ˜¯ä¸æ˜¯ä¼ªç§‘å­¦ï¼Ÿ](https://www.zhihu.com/question/587177332)


## æ•°å­¦å®¶
æ‡’å¾—åˆ†ç±»äº†ï¼Œéšä¾¿æ–°åŠ äº†ä¸€ç±»
1. [æ—åŠ›è¡ŒhÃ¡ngï¼ˆLek-Heng Limï¼‰](http://www.stat.uchicago.edu/~lekheng/work/reprints.html)ä½¿ç”¨ä»£æ•°ã€å‡ ä½•å’Œæ‹“æ‰‘å·¥å…·æ¥å›ç­”æœºå™¨å­¦ä¹ ä¸­çš„é—®é¢˜ã€‚ä¸€ç¯‡é‡‡è®¿[quantamagazineé‡‡è®¿](https://www.quantamagazine.org/an-applied-mathematician-strengthens-ai-with-pure-math-20230301/)å’Œ[ä¸­æ–‡ç‰ˆ](https://mp.weixin.qq.com/s/5zFz0GUo0VB7hJMI4PxvVA) 
    + 1.1  [Topology of deep neural networks](https://dl.acm.org/doi/abs/10.5555/3455716.3455900) å°†ç¥ç»ç½‘ç»œè¡¨è¾¾çš„ç‰©ä½“è§†ä¸ºæ‹“æ‰‘æµå½¢ï¼Œä¸åŒç±»åˆ«åœ¨ç›¸ç‰‡å±‚é¢å…·æœ‰å¾ˆå¤§ç›¸ä¼¼åº¦çš„æµå½¢ä¼šä»¥éå¸¸å¤æ‚çš„æ–¹å¼äº¤ç»‡åœ¨ä¸€èµ·ï¼Œä½œè€…è¿›è¡Œäº†å®éªŒï¼Œè¯æ˜è¿™äº›æµå½¢èƒ½è¢«ç®€åŒ–ï¼Œå¹¶åˆ©ç”¨è®¡ç®—æ‹“æ‰‘ä¸­çš„æŒç»­åŒè°ƒpersistent homologyæ¥æµ‹é‡è¿™äº›ç‰©ä½“çš„å½¢çŠ¶ã€‚ç®€å•æ¥è¯´ï¼Œä½œè€…ç”¨å®ƒæ¥æµ‹é‡æµå½¢ç©¿è¿‡ç¥ç»ç½‘ç»œå±‚æ—¶çš„å½¢çŠ¶ã€‚æœ€ç»ˆï¼Œè¯æ˜å®ƒç®€åŒ–ä¸ºæœ€ç®€å•çš„å½¢å¼ã€‚è¿™å¯¹ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§æœ‰å¸®åŠ©ã€‚
    + 1.2 [Recht-RÃ© Noncommutative Arithmetic-Geometric Mean Conjecture is False](https://arxiv.org/abs/2006.01510)æˆ‘çš„åšå£«ç”ŸZehua Laiå’Œæˆ‘å±•ç¤ºäº†æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„çŒœæƒ³æ˜¯é”™è¯¯çš„ã€‚â€œç°ä»£æœºå™¨å­¦ä¹ é—®é¢˜é€šå¸¸æ¶‰åŠå°†å¤§é‡å‚æ•°ä¸å¤§é‡æ•°æ®æ‹Ÿåˆã€‚GPT-4æ˜¯ChatGPTåº•å±‚å¼•æ“çš„ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Œæ®ä¼ æœ‰1ä¸‡äº¿åˆ°100ä¸‡äº¿ä¸ªå‚æ•°ã€‚ç°æœ‰çš„è®¡ç®—æœºæ— æ³•åŒæ—¶å¤„ç†è¿™äº›å‚æ•°ã€‚å› æ­¤ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•éƒ½ä¼šéšæœºé€‰æ‹©ä¸€å°éƒ¨åˆ†å‚æ•°ï¼ˆæ— è®ºè®¡ç®—æœºå¯ä»¥å¤„ç†ä»€ä¹ˆï¼‰ï¼Œç„¶ååªä½¿ç”¨è¿™äº›å‚æ•°ã€‚é€‰å–ä¸€ä¸ªå°çš„éšæœºå­é›†ç§°ä¸ºæŠ½æ ·ï¼ˆå–æ ·ï¼‰ã€‚ç°åœ¨çš„é—®é¢˜æ˜¯ï¼šåœ¨ç®—æ³•çš„åç»­æ­¥éª¤ä¸­ï¼Œå®ƒåº”è¯¥é€‰æ‹©æˆ‘ä»¬ä¹‹å‰åœ¨å‰é¢çš„æ­¥éª¤ä¸­å·²ç»é€‰æ‹©çš„å‚æ•°ï¼Œè¿˜æ˜¯åº”è¯¥æ’é™¤è¿™äº›å‚æ•°ï¼Ÿæ¢å¥è¯è¯´ï¼Œå®ƒåº”è¯¥å¯¹å‚æ•°è¿›è¡Œæ›¿æ¢è¿˜æ˜¯ä¸æ›¿æ¢ï¼Ÿå½“æˆ‘ä»¬çš„ç®—æ³•æ¶‰åŠéšæœºåŒ–æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬æ€»æ˜¯éœ€è¦è€ƒè™‘çš„é—®é¢˜ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬å’Œé‡è¦çš„é—®é¢˜ã€‚å¤§çº¦10å¹´å‰ï¼ŒBen Rechtå’ŒChris RÃ©è¡¨æ˜ï¼Œä¸æ›¿æ¢é‡‡æ ·æ¯”æ›¿æ¢æ›´å¥½ï¼Œå‰ææ˜¯ç‰¹å®šä¸å¹³ç­‰çš„æŸç§ç±»ä¼¼ç‰©æˆç«‹ã€‚å¤šå¹´æ¥ï¼Œäººä»¬è¯æ˜äº†è¿™ç§ä¸å¹³ç­‰çš„å„ç§æ¡ˆä¾‹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ€»çš„æ¥è¯´ï¼Œè¿™ç§ä¸å¹³ç­‰å¹¶ä¸æˆç«‹ã€‚å›ç­”è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä»£æ•°å‡ ä½•ä¸­çš„ä¸€ç§ç§°ä¸ºéäº¤æ¢æ­£ç‚¹ï¼ˆnoncommutative Positivstellensatzï¼‰çš„å·¥å…·ã€‚è¿™æ˜¯ä¸€ä¸ªåˆé•¿åˆæ‹—å£çš„è¯ã€‚å®ƒæ˜¯ä¸€ä¸ªå¾·è¯­å•è¯ï¼Œæœ¬è´¨ä¸Šæ„å‘³ç€å¤šé¡¹å¼æ­£ç‚¹çš„ä½ç½®ã€‚â€œ
    + 1.3 [LU decomposition and Toeplitz decomposition of a neural network](http://www.stat.uchicago.edu/~lekheng/work/lu.pdf)æ‘˜è¦: It is well-known that any matrix $A$ has an LU decomposition. Less well-known is the fact that it has a 'Toeplitz decomposition' $A=T_1 T_2 \cdots T_r$ where $T_i$ 's are Toeplitz matrices. We will prove that any continuous function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ has an approximation to arbitrary accuracy by a neural network that takes the form $L_1 \sigma_1 U_1 \sigma_2 L_2 \sigma_3 U_2 \cdots L_r \sigma_{2 r-1} U_r$, i.e., where the weight matrices alternate between lower and upper triangular matrices, $\sigma_i(x):=\sigma\left(x-b_i\right)$ for some bias vector $b_i$, and the activation $\sigma$ may be chosen to be essentially any uniformly continuous nonpolynomial function. The same result also holds with Toeplitz matrices, i.e., $f \approx T_1 \sigma_1 T_2 \sigma_2 \cdots \sigma_{r-1} T_r$ to arbitrary accuracy, and likewise for Hankel matrices. A consequence of our Toeplitz result is a fixed-width universal approximation theorem for convolutional neural networks, which so far have only arbitrary width versions. Since our results apply in particular to the case when $f$ is a general neural network, we may regard them as LU and Toeplitz decompositions of a neural network. The practical implication of our results is that one may vastly reduce the number of weight parameters in a neural network without sacrificing its power of universal approximation. We will present several experiments on real data sets to show that imposing such structures on the weight matrices sharply reduces the number of training parameters with almost no noticeable effect on test accuracy.
    + 1.4 [What is â€¦ an equivariant neural network?](http://www.stat.uchicago.edu/~lekheng/work/equivariant.pdf )æ–‡ç« è¯æ˜äº†AlphaFold2å’Œ[ImageNet classification with deep convolutional neural networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)åœ¨æ²¡æœ‰ç¥ç»ç½‘ç»œçš„ç­‰å˜æ€§å‡è®¾æƒ…å†µä¸‹æ˜¯ç­‰ä»·(Equivariant)çš„ã€‚

## è¿‘ä¼¼
1. [ å‹ç¼©ä¸‹ä¸€ä¸ª token é€šå‘è¶…è¿‡äººç±»çš„æ™ºèƒ½](https://zhuanlan.zhihu.com/p/619511222)(2023)
2. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)è®ºæ–‡æå‡ºè¯­è¨€æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦éšç€å‚æ•°æ•°é‡çš„å¢åŠ å‘ˆçº¿æ€§å…³ç³»,è€Œä¸æ˜¯å¹³æ–¹å…³ç³»ã€‚è¿™ä¸ºå·¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†ç†è®ºæ”¯æŒã€‚(2020)
3. [æ‰©æ•£æ¨¡å‹ä¸èƒ½é‡æ¨¡å‹ï¼ŒScore-Matchingå’ŒSDEï¼ŒODEçš„å…³ç³»](https://zhuanlan.zhihu.com/p/576779879)å¯¹æ‰©æ•£æ¨¡å‹åŒéšæœºå¾®åˆ†æ–¹ç¨‹ç­‰çš„è”ç³»ï¼Œåšäº†æ¯”è¾ƒæ¸…æ™°çš„æ¢³ç†(2022)ã€‚æ›´è¯¦ç»†çš„å‚è€ƒ[What are Diffusion Models? ](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)(2021)ã€‚
4. [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)è¯¥è®ºæ–‡ä»ä¼˜åŒ–çš„è§’åº¦å‘ç°GPTé¢„è®­ç»ƒå®é™…ä¸ŠåŒæ—¶è¿›è¡Œä¸¤å±‚ä¼˜åŒ–:ä¸€å±‚è¿›è¡Œè¯­è¨€æ¨¡å‹å‚æ•°å­¦ä¹ ,ä¸€å±‚è¿›è¡Œå…ƒä¼˜åŒ–ä»¥ç”Ÿæˆæœ€ä¼˜åºåˆ—ã€‚è¿™ç§åµŒå¥—ä¼˜åŒ–æœºåˆ¶èµ‹äºˆGPTå¼ºå¤§çš„æ³›åŒ–å’Œè‡ªé€‚åº”èƒ½åŠ›ã€‚è¿™ä¸ºç†è§£è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒåŸç†å’Œæ³›åŒ–æ€§èƒ½æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚(2022.12)
5. [GPT-PINN: ç”Ÿæˆå¼é¢„è®­ç»ƒå†…åµŒç‰©ç†çŸ¥è¯†ç¥ç»ç½‘ç»œ ](https://mp.weixin.qq.com/s/Gfbl5p1aISwneEbJvMhneQ)CAMä¼ ä¹ å½•(2023.4)ã€‚
6. [ä» Transformer åˆ°è¾¹ç•Œå€¼åé—®é¢˜ï¼ˆä¸€ï¼‰ ](https://mp.weixin.qq.com/s/lDIIfQ_ngM_o7GblrA3jlQ)CAMä¼ ä¹ å½•(2023.4)ã€‚
7. [Defining and Quantifying the Emergence of Sparse Concepts in DNNs](https://arxiv.org/abs/2111.06206)ç¥ç»ç½‘ç»œä¸­çš„ç¬¦å·æ¦‚å¿µæ¶Œç°ï¼Œå…¶ä¸­æ–‡å®˜æ–¹ä»‹ç»[AIä»æŠ€æœ¯åˆ°ç§‘å­¦ï¼šç¥ç»ç½‘ç»œä¸­çš„æ¦‚å¿µç¬¦å·æ¶Œç°çš„å‘ç°ä¸è¯æ˜](https://zhuanlan.zhihu.com/p/618870800)(2023.4)
