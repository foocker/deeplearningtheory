# Deep Learning Theory
æ•´ç†äº†ä¸€äº›æ·±åº¦å­¦ä¹ çš„ç†è®ºç›¸å…³å†…å®¹ï¼ŒæŒç»­æ›´æ–°ã€‚
## Overview
1. [Recent advances in deep learning theory](https://arxiv.org/pdf/2012.10931.pdf)
æ€»ç»“äº†ç›®å‰æ·±åº¦å­¦ä¹ ç†è®ºç ”ç©¶çš„å…­ä¸ªæ–¹å‘çš„ä¸€äº›ç»“æœï¼Œæ¦‚è¿°å‹ï¼Œæ²¡åšæ·±å…¥æ¢è®¨(2021)ã€‚

    + 1.1 complexity and capacity-basedapproaches for analyzing the generalizability of deep learning; 

    + 1.2 stochastic differential equations andtheir dynamic systems for modelling stochastic gradient descent and its variants, which characterizethe optimization and generalization of deep learning, partially inspired by Bayesian inference; 

    + 1.3 thegeometrical structures of the loss landscape that drives the trajectories of the dynamic systems;

    + 1.4 theroles of over-parameterization of deep neural networks from both positive and negative perspectives; 

    + 1.5 theoretical foundations of several special structures in network architectures; 

    + 1.6 the increasinglyintensive concerns in ethics and security and their relationships with generalizability

## Course
2. [Theory of Deep Learning](https://www.ideal.northwestern.edu/special-quarters/fall-2020/)TTIC,è¥¿åŒ—å¤§å­¦ç­‰ç»„ç»‡çš„ä¸€ç³»åˆ—è¯¾ç¨‹å’Œè®²åº§ï¼ŒåŸºç¡€è¯¾ç¨‹æ¶‰åŠDLçš„åŸºç¡€(ç¬¦å·åŒ–ï¼Œç®€åŒ–åçš„æ•°å­¦é—®é¢˜å’Œç»“è®º)ï¼Œä¿¡æ¯è®ºå’Œå­¦ä¹ ï¼Œç»Ÿè®¡å’Œè®¡ç®—ï¼Œä¿¡æ¯è®ºï¼Œç»Ÿè®¡å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ (2020)ã€‚

3. [MathsDL-spring19](https://joanbruna.github.io/MathsDL-spring19/),MathDLç³»åˆ—ï¼Œ18,19,20å¹´å‡æœ‰ã€‚

    + 3.1  Geometry of Data

        + Euclidean Geometry: transportation metrics, CNNs , scattering.
        + Non-Euclidean Geometry: Graph Neural Networks.
        + Unsupervised Learning under Geometric Priors (Implicit vs explicit models, microcanonical, transportation metrics).
        + Applications and Open Problems: adversarial examples, graph inference, inverse problems.

    + 3.2 Geometry of Optimization and Generalization

        + Stochastic Optimization (Robbins & Munro, Convergence of SGD)
        + Stochastic Differential Equations (Fokker-Plank, Gradient Flow, Langevin + + Dynamics, links with SGD; open problems)
        Dynamics of Neural Network Optimization (Mean Field Models using Optimal Transport, Kernel Methods)
        + Landscape of Deep Learning Optimization (Tensor/Matrix factorization, Deep Nets; open problems).
        + Generalization in Deep Learning.

    + 3.3  Open qustions on Reinforcement Learning
4. [IFT 6169: Theoretical principles for deep learning](http://mitliagkas.github.io/ift6085-dl-theory-class/)(2022 Winter),å¤§å¤šå†…å®¹è¾ƒä¸ºåŸºç¡€ï¼Œä¼ ç»Ÿã€‚
    + 4.1 æ‹Ÿå®šè¯¾é¢˜
        + Generalization: theoretical analysis and practical bounds
        + Information theory and its applications in ML (information bottleneck, lower bounds etc.)
        +  Generative models beyond the pretty pictures: a tool for traversing the data manifold, projections, completion, substitutions etc.
        + Taming adversarial objectives: Wasserstein GANs, regularization approaches and + controlling the dynamics
        + The expressive power of deep networks (deep information propagation, mean-field analysis of random networks etc.)
        


## Architecture
5. [Partial Differential Equations is All You Need for Generating Neural Architectures -- A Theory for Physical Artificial Intelligence Systems](https://arxiv.org/abs/2103.08313) å°†ç»Ÿè®¡ç‰©ç†çš„ååº”æ‰©æ•£æ–¹ç¨‹ï¼Œé‡å­åŠ›å­¦ä¸­çš„è–›å®šè°”æ–¹ç¨‹ï¼Œå‚è½´å…‰å­¦ä¸­çš„äº¥å§†éœå…¹æ–¹ç¨‹ç»Ÿä¸€æ•´åˆåˆ°ç¥ç»ç½‘ç»œåå¾®åˆ†æ–¹ç¨‹ä¸­(NPDE)ï¼Œåˆ©ç”¨æœ‰é™å…ƒæ–¹æ³•æ‰¾åˆ°æ•°å€¼è§£ï¼Œä»ç¦»æ•£è¿‡ç¨‹ä¸­ï¼Œæ„é€ äº†å¤šå±‚æ„ŸçŸ¥ï¼Œå·ç§¯ç½‘ç»œï¼Œå’Œå¾ªç¯ç½‘ç»œï¼Œå¹¶æä¾›äº†ä¼˜åŒ–æ–¹æ³•L-BFGSç­‰ï¼Œä¸»è¦æ˜¯å»ºç«‹äº†ç»å…¸ç‰©ç†æ¨¡å‹å’Œç»å…¸ç¥ç»ç½‘ç»œçš„è”ç³»(2021)ã€‚

## Approximation
6. NN Approximation Theory
    + 6.0 [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)NNé€¼è¿‘ä»è¿™é‡Œå¼€å§‹(1991)
    + 6.1 [Cybenkoâ€™s Theorem and the capabilityof a neural networkas function approximator](https://www.mathematik.uni-wuerzburg.de/fileadmin/10040900/2019/Seminar__Artificial_Neural_Network__24_9__.pdf)ä¸€äºŒç»´shallowç¥ç»ç½‘ç»œçš„å¯è§†åŒ–è¯æ˜(2019)
    + 6.2 [Depth Separation for Neural Networks](https://arxiv.org/abs/1702.08489) ä¸‰å±‚ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›æ¯”ä¸¤å±‚æœ‰ä¼˜è¶Šæ€§çš„ç®€åŒ–è¯æ˜ (2017)
    + 6.3 [èµµæ‹“ç­‰ä¸‰ç¯‡](https://www.zhihu.com/question/347654789/answer/1480974642)(2019)
    + 6.4 [Neural Networks with Small Weights and Depth-Separation Barriers](https://arxiv.org/abs/2006.00625) Gal Vardiç­‰è¯æ˜äº†å¯¹æŸäº›ç±»å‹çš„ç¥ç»ç½‘ç»œ, ç”¨kå±‚çš„å¤šé¡¹å¼è§„æ¨¡ç½‘ç»œéœ€è¦ä»»æ„weight, ä½†ç”¨3k+3å±‚çš„å¤šé¡¹å¼è§„æ¨¡ç½‘ç»œåªéœ€è¦å¤šé¡¹å¼å¤§å°çš„ weight(2020)ã€‚
    + 6.5 [Universality of Deep Convolutional Neural Networks](https://arxiv.org/pdf/1805.10769.pdf)å·ç§¯ç½‘ç»œçš„é€šç”¨é€¼è¿‘èƒ½åŠ›ï¼ŒåŠå…¶æ ¸å¿ƒè¦ç´ ï¼ŒDing-Xuan Zhou(2018)ï¼Œ20å¹´å‘è¡¨ã€‚

## Optimization
7. SGD
    + 7.1 [nesterov-accelerated-gradient](https://paperswithcode.com/method/nesterov-accelerated-gradient)

8. [offconvex](http://www.offconvex.org/)å‡ ä¸ªå­¦æœ¯å·¥ä½œè€…ç»´æŠ¤çš„AIåšå®¢ã€‚
    + 8.1 [beyondNTK](http://www.offconvex.org/2021/03/25/beyondNTK/) ä»€ä¹ˆæ—¶å€™NNå¼ºäºNTKï¼Ÿ
    + 8.2 [instahide](http://www.offconvex.org/2020/11/11/instahide/) å¦‚ä½•åœ¨ä¸æ³„éœ²æ•°æ®çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨¡å‹ï¼Ÿ
    + 8.3 [implicit regularization in DL explained by norms?](http://www.offconvex.org/2020/11/27/reg_dl_not_norm/)

## Geometry
9. Optima transmission
    + 9.1 [æ·±åº¦å­¦ä¹ çš„å‡ ä½•å­¦è§£é‡Š](http://www.engineering.org.cn/ch/10.1016/j.eng.2019.09.010)(2020)

## Book
10. [Theory of Deep Learning(draft)](https://www.cs.princeton.edu/courses/archive/fall19/cos597B/lecnotes/bookdraft.pdf)Rong Ge ç­‰(2019)ã€‚

11. [Spectral Learning on Matrices and Tensors](https://arxiv.org/pdf/2004.07984.pdf)Majid Janzaminç­‰(2020)

19. [Deep Learning Architectures A Mathematical Approach](https://www.springer.com/gp/book/9783030367206)(2020),ä½ å¯ä»¥libgenè·å–ï¼Œå†…å®¹å¦‚å…¶åå­—,å¤§æ¦‚åŒ…å«ï¼šå·¥ä¸šé—®é¢˜ï¼ŒDLåŸºç¡€(æ¿€æ´»ï¼Œç»“æ„ï¼Œä¼˜åŒ–ç­‰),å‡½æ•°é€¼è¿‘ï¼Œä¸‡æœ‰é€¼è¿‘ï¼ŒRELUç­‰é€¼è¿‘æ–°ç ”ç©¶ï¼Œå‡½æ•°è¡¨ç¤ºï¼Œä»¥åŠä¸¤å¤§æ–¹å‘ï¼Œä¿¡æ¯è§’åº¦ï¼Œå‡ ä½•è§’åº¦ç­‰ç›¸å…³çŸ¥è¯†ï¼Œå®é™…åœºæ™¯ä¸­çš„å·ç§¯ï¼Œæ± åŒ–ï¼Œå¾ªç¯ï¼Œç”Ÿæˆï¼Œéšæœºç½‘ç»œç­‰å…·ä½“å®ç”¨å†…å®¹çš„æ•°å­¦åŒ–ï¼Œå¦å¤–é™„å½•é›†åˆè®ºï¼Œæµ‹åº¦è®ºï¼Œæ¦‚ç‡è®ºï¼Œæ³›å‡½ï¼Œå®åˆ†æç­‰åŸºç¡€çŸ¥è¯†ã€‚
20. [The Principles of Deep Learning Theory](https://arxiv.org/pdf/2106.10165.pdf)(2021)Daniel A. Roberts and Sho Yaida(mit)ï¼ŒBeginning from a first-principles component-level picture of networksï¼Œæœ¬ä¹¦è§£é‡Šäº†å¦‚ä½•é€šè¿‡æ±‚è§£å±‚åˆ°å±‚è¿­ä»£æ–¹ç¨‹å’Œéçº¿æ€§å­¦ä¹ åŠ¨åŠ›å­¦æ¥ç¡®å®šè®­ç»ƒç½‘ç»œè¾“å‡ºçš„å‡†ç¡®æè¿°ã€‚ä¸€ä¸ªä¸»è¦çš„ç»“æœæ˜¯ç½‘ç»œçš„é¢„æµ‹æ˜¯ç”±è¿‘é«˜æ–¯åˆ†å¸ƒæè¿°çš„ï¼Œç½‘ç»œçš„æ·±åº¦ä¸å®½åº¦çš„çºµæ¨ªæ¯”æ§åˆ¶ç€ä¸æ— é™å®½åº¦é«˜æ–¯æè¿°çš„åå·®ã€‚æœ¬ä¹¦è§£é‡Šäº†è¿™äº›æœ‰æ•ˆæ·±åº¦ç½‘ç»œå¦‚ä½•ä»è®­ç»ƒä¸­å­¦ä¹ éå¹³å‡¡çš„è¡¨ç¤ºï¼Œå¹¶æ›´å¹¿æ³›åœ°åˆ†æéçº¿æ€§æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ æœºåˆ¶ã€‚ä»è¿‘å†…æ ¸æ–¹æ³•çš„è§’åº¦æ¥çœ‹ï¼Œå‘ç°è¿™äº›æ¨¡å‹çš„é¢„æµ‹å¯¹åº•å±‚å­¦ä¹ ç®—æ³•çš„ä¾èµ–å¯ä»¥ç”¨ä¸€ç§ç®€å•è€Œé€šç”¨çš„æ–¹å¼æ¥è¡¨è¾¾ã€‚ä¸ºäº†è·å¾—è¿™äº›ç»“æœï¼Œä½œè€…å¼€å‘äº†è¡¨ç¤ºç»„æµï¼ˆRG æµï¼‰çš„æ¦‚å¿µæ¥è¡¨å¾ä¿¡å·é€šè¿‡ç½‘ç»œçš„ä¼ æ’­ã€‚é€šè¿‡å°†ç½‘ç»œè°ƒæ•´åˆ°ä¸´ç•ŒçŠ¶æ€ï¼Œä»–ä»¬ä¸ºæ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜æä¾›äº†ä¸€ä¸ªå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ä½œè€…è¿›ä¸€æ­¥è§£é‡Šäº† RG æµå¦‚ä½•å¯¼è‡´è¿‘ä¹æ™®éçš„è¡Œä¸ºï¼Œä»è€Œå¯ä»¥å°†ç”±ä¸åŒæ¿€æ´»å‡½æ•°æ„å»ºçš„ç½‘ç»œåšç±»åˆ«åˆ’åˆ†ã€‚Altogether, they show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networksã€‚åˆ©ç”¨ä¿¡æ¯ç†è®ºï¼Œä½œè€…ä¼°è®¡äº†æ¨¡å‹æ€§èƒ½æœ€å¥½çš„æœ€ä½³æ·±å®½æ¯”ï¼Œå¹¶è¯æ˜äº†æ®‹å·®è¿æ¥èƒ½å°†æ·±åº¦æ¨å‘ä»»æ„æ·±åº¦ã€‚åˆ©ç”¨ä»¥ä¸Šç†è®ºå·¥å…·ï¼Œå°±å¯ä»¥æ›´åŠ ç»†è‡´çš„ç ”ç©¶æ¶æ„çš„å½’çº³åå·®ï¼Œè¶…å‚æ•°ï¼Œä¼˜åŒ–ã€‚[åŸä½œè€…çš„è§†é¢‘è¯´æ˜](https://www.youtube.com/watch?v=wXZKoHEzASg)(2021.12.1)
21. [Physics-based Deep Learning](https://arxiv.org/pdf/2109.05237.pdf)(2021)N. Thuerey, P. Holl,etc.[github resources](https://github.com/thunil/Physics-Based-Deep-Learning)æ·±åº¦å­¦ä¹ ä¸ç‰©ç†å­¦çš„è”ç³»ã€‚æ¯”å¦‚åŸºäºç‰©ç†çš„æŸå¤±å‡½æ•°ï¼Œå¯å¾®æµä½“æ¨¡æ‹Ÿï¼Œé€†é—®é¢˜çš„æ±‚è§£ï¼ŒNavier-Stokesæ–¹ç¨‹çš„å‰å‘æ¨¡æ‹Ÿï¼ŒControlling Burgersâ€™ Equationå’Œå¼ºåŒ–å­¦ä¹ çš„å…³ç³»ç­‰ã€‚

## Session
21. [Foundations of Deep Learning](https://simons.berkeley.edu/programs/dl2019)(2019)ï¼Œè¥¿è’™ç ”ç©¶ä¸­å¿ƒä¼šè®®ã€‚
22. [Deep Learning Theory 4](https://icml.cc/virtual/2021/session/12048)(2021, ICML)Claire Monteleoniä¸»æŒ...,æ·±åº¦å­¦ä¹ ç†è®ºä¼šè®®4ï¼ŒåŒ…å«è®ºæ–‡å’Œè§†é¢‘ã€‚
23. [Deep Learning Theory 5 ](https://icml.cc/virtual/2021/session/12057)(2021,ICML)MaYiä¸»æŒ...ï¼Œæ·±åº¦å­¦ä¹ ç†è®ºä¼šè®®5ï¼ŒåŒ…å«è®ºæ–‡å’Œè§†é¢‘ã€‚

## generalization
1. [Robust Learning with Jacobian Regularization](https://arxiv.org/abs/1908.02729)(2019)Judy Hoffman..., 
2. [Predicting Generalization using GANs](http://www.offconvex.org/2022/06/06/PGDL/)(2022.6),ç”¨GANæ¥è¯„ä¼°æ³›åŒ–æ€§.
3. [Implicit Regularization in Tensor Factorization: Can Tensor Rank Shed Light on Generalization in Deep Learning?](http://www.offconvex.org/2021/07/08/imp-reg-tf/)(2021.7)Tensor Rank èƒ½å¦æ­ç¤ºæ·±åº¦å­¦ä¹ ä¸­çš„æ³›åŒ–ï¼Ÿ
4. [å¦‚ä½•é€šè¿‡Meta Learningå®ç°åŸŸæ³›åŒ–Domain Generalization](https://mp.weixin.qq.com/s/o1liWf9B4_LntBeyV2bVOg)(2022.4),[Domain Generalization CVPR2022](https://mp.weixin.qq.com/s/HkjHEqs8d85VPdgpaHEzPQ)åšæ–‡å‚è€ƒ.


## Others
4. [Theoretical issues in deep networks](https://www.pnas.org/content/117/48/30039) è¡¨æ˜æŒ‡æ•°å‹æŸå¤±å‡½æ•°ä¸­å­˜åœ¨éšå¼çš„æ­£åˆ™åŒ–ï¼Œå…¶ä¼˜åŒ–çš„ç»“æœå’Œä¸€èˆ¬æŸå¤±å‡½æ•°ä¼˜åŒ–ç»“æœä¸€è‡´ï¼Œä¼˜åŒ–æ”¶æ•›ç»“æœå’Œæ¢¯åº¦æµçš„è¿¹æœ‰å…³ï¼Œç›®å‰è¿˜ä¸èƒ½è¯æ˜å“ªä¸ªç»“æœæœ€ä¼˜(2020)ã€‚
12. [The Dawning of a New Erain Applied Mathematics](https://www.ams.org/journals/notices/202104/rnoti-p565.pdf)Weinan Eå…³äºåœ¨DLçš„æ–°å¤„å¢ƒä¸‹ç»“åˆå†å²çš„å·¥ä½œèŒƒå¼ç»™å‡ºçš„æŒ‡å¯¼æ€§æ€»ç»“(2021)ã€‚
13. [Mathematics of deep learning from Newton Institute](https://www.newton.ac.uk/event/mdl)ã€‚
14. [DEEP NETWORKS FROM THE PRINCIPLE OF RATE REDUCTION](https://openreview.net/forum?id=G70Z8ds32C9)ï¼Œç™½ç›’ç¥ç»ç½‘ç»œã€‚
15. [redunet_paper](https://github.com/ryanchankh/redunet_paper)ç™½ç›’ç¥ç»ç½‘ç»œä»£ç ã€‚
16. [Theory of Deep Convolutional Neural Networks:Downsampling](https://www.cityu.edu.hk/rcms/pdf/XDZhou/dxZhou2020b.pdf)ä¸‹é‡‡æ ·çš„æ•°å­¦åˆ†æDing-Xuan Zhou(2020)
17. [Theory of deep convolutional neural networks II: Spherical analysis](https://arxiv.org/abs/2007.14285)è¿˜æœ‰IIIï¼šradial functions é€¼è¿‘ï¼Œ(2020)ã€‚ä¸è¿‡è¿™äº›å·¥ä½œåˆ°åº•å¦‚ä½•ï¼Œåªæ˜¯ç”¨æ•°å­¦è½¬æ¢äº†ä¸€ä¸‹ï¼Œç†è®ºä¸Šæ²¡åšè¿‡å¤šè´¡çŒ®ï¼Œæˆ–è€…å’Œå®é™…ç»“åˆæ²¡éš¾ä¹ˆç´§å¯†ï¼Œè¿˜ä¸å¾—è€ŒçŸ¥ã€‚
18. [The Modern Mathematics of Deep Learning](https://arxiv.org/abs/2105.04026)(2021)ä¸»è¦æ˜¯deep laerningçš„æ•°å­¦åˆ†ææè¿°ï¼Œæ¶‰åŠçš„é—®é¢˜åŒ…æ‹¬ï¼šè¶…å‚æ•°ç½‘ç»œçš„é€šç”¨èƒ½åŠ›ï¼Œæ·±åº¦åœ¨æ·±åº¦æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ·±åº¦å­¦ä¹ å¯¹ç»´åº¦ç¾éš¾çš„å…‹æœï¼Œä¼˜åŒ–åœ¨éå‡¸ä¼˜åŒ–é—®é¢˜çš„æˆåŠŸï¼Œå­¦ä¹ çš„è¡¨ç¤ºç‰¹å¾çš„æ•°å­¦åˆ†æï¼Œä¸ºä½•æ·±åº¦æ¨¡å‹åœ¨ç‰©ç†é—®é¢˜ä¸Šæœ‰è¶…å¸¸è¡¨ç°ï¼Œæ¨¡å‹æ¶æ„ä¸­çš„å“ªäº›å› ç´ ä»¥ä½•ç§æ–¹å¼å½±å“ä¸åŒä»»åŠ¡çš„å­¦ä¹ ä¸­çš„ä¸åŒæ–¹é¢ã€‚
19. [Topos and Stacks of Deep Neural Networks](https://arxiv.org/abs/2106.14587)(2021)æ¯ä¸€ä¸ªå·²çŸ¥çš„æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)å¯¹åº”äºä¸€ä¸ªå…¸å‹çš„ Grothendieck çš„ topos ä¸­çš„ä¸€ä¸ªå¯¹è±¡; å®ƒçš„å­¦ä¹ åŠ¨æ€å¯¹åº”äºè¿™ä¸ª topos ä¸­çš„ä¸€ä¸ªæ€å°„æµã€‚å±‚ä¸­çš„ä¸å˜æ€§ç»“æ„(å¦‚ CNNs æˆ– LSTMs)ä¸Giraud's stacksç›¸å¯¹åº”ã€‚è¿™ç§ä¸å˜æ€§è¢«è®¤ä¸ºæ˜¯æ³›åŒ–æ€§è´¨çš„åŸå› ï¼Œå³ä»çº¦æŸæ¡ä»¶ä¸‹çš„å­¦ä¹ æ•°æ®è¿›è¡Œæ¨æ–­ã€‚çº¤ç»´ä»£è¡¨å‰è¯­ä¹‰ç±»åˆ«(Culioliï¼ŒThom) ï¼Œå…¶ä¸Šäººå·¥è¯­è¨€çš„å®šä¹‰ï¼Œå†…éƒ¨é€»è¾‘ï¼Œç›´è§‰ä¸»ä¹‰ï¼Œç»å…¸æˆ–çº¿æ€§(Girard)ã€‚ç½‘ç»œçš„è¯­ä¹‰åŠŸèƒ½æ˜¯ç”¨è¿™ç§è¯­è¨€è¡¨è¾¾ç†è®ºçš„èƒ½åŠ›ï¼Œç”¨äºå›ç­”è¾“å…¥æ•°æ®è¾“å‡ºä¸­çš„é—®é¢˜ã€‚è¯­ä¹‰ä¿¡æ¯çš„é‡å’Œç©ºé—´çš„å®šä¹‰ä¸é¦™å†œç†µçš„åŒæºè§£é‡Šç›¸ç±»ä¼¼ã€‚ä»–ä»¬æ¨å¹¿äº† Carnap å’Œ Bar-Hillel (1952)æ‰€å‘ç°çš„åº¦é‡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸Šè¿°è¯­ä¹‰ç»“æ„è¢«åˆ†ç±»ä¸ºå‡ ä½•çº¤ç»´å¯¹è±¡åœ¨ä¸€ä¸ªå°é—­çš„Quillenæ¨¡å‹èŒƒç•´ï¼Œç„¶åä»–ä»¬å¼•èµ·åŒæ—¶å±€éƒ¨ä¸å˜çš„ dnn å’Œä»–ä»¬çš„è¯­ä¹‰åŠŸèƒ½ã€‚Intentional type theories (Martin-Loef)ç»„ç»‡è¿™äº›å¯¹è±¡å’Œå®ƒä»¬ä¹‹é—´çš„çº¤ç»´åŒ–ã€‚ä¿¡æ¯å†…å®¹å’Œäº¤æ¢ç”± Grothendieck's derivatorsåˆ†æã€‚
20. [Visualizing the Emergence of Intermediate Visual Patterns in DNNs](https://arxiv.org/abs/2111.03505)(2021,NIPS)æ–‡ç« è®¾è®¡äº†ä¸€ç§ç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„å¯è§†åŒ–æ–¹æ³•ï¼Œä½¿å¾—èƒ½
ï¼ˆ1ï¼‰æ›´ç›´è§‚åœ°åˆ†æç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶ä¸”å±•ç¤ºä¸­å±‚ç‰¹å¾è¡¨è¾¾èƒ½åŠ›çš„æ—¶ç©ºæ¶Œç°ï¼›
ï¼ˆ2ï¼‰é‡åŒ–ç¥ç»ç½‘ç»œä¸­å±‚çŸ¥è¯†ç‚¹ï¼Œä»è€Œå®šé‡åœ°åˆ†æç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„è´¨é‡ï¼›
ï¼ˆ3ï¼‰ä¸ºä¸€äº›æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚å¯¹æŠ—æ”»å‡»ã€çŸ¥è¯†è’¸é¦ï¼‰æä¾›æ–°è§è§£ã€‚
21.  [ç¥ç»ç½‘ç»œçš„åšå¼ˆäº¤äº’è§£é‡Šæ€§](https://zhuanlan.zhihu.com/p/264871522/)(çŸ¥ä¹)ã€‚ä¸Šäº¤å¤§å¼ æ‹³çŸ³å›¢é˜Ÿç ”ç©¶è®ºæ–‡æ•´ç†è€Œå¾—ï¼Œä½œä¸ºåšå¼ˆäº¤äº’è§£é‡Šæ€§çš„ä½“ç³»æ¡†æ¶ï¼ˆä¸æ€ä¹ˆç¨³å›ºï¼‰ã€‚
22.  [Advancing mathematics by guiding human intuition with AI](https://www.nature.com/articles/s41586-021-04086-x)(2021,nature)æœºå™¨å­¦ä¹ å’Œæ•°å­¦å®¶å·¥ä½œçš„ä¸€ä¸ªæœ‰æœºç»“åˆï¼Œä¸»è¦åˆ©ç”¨æœºå™¨å­¦ä¹ åˆ†æä¼—å¤šç‰¹å¾å’Œç›®æ ‡å˜é‡çš„ä¸»è¦ç›¸å…³å› å­ï¼ŒåŠ å¼ºæ•°å­¦å®¶çš„ç›´è§‰ï¼Œè¯¥è®ºæ–‡å¾—åˆ°äº†ä¸¤ä¸ªæ¼‚äº®çš„å®šç†ï¼Œä¸€ä¸ªæ‹“æ‰‘ï¼Œä¸€ä¸ªè¡¨ç¤ºè®ºã€‚å¯å‚è€ƒ[å›ç­”](https://www.zhihu.com/question/503185412/answer/2256015652)ã€‚
23. ğŸ”¥[A New Perspective of Entropy](https://math3ma.institute/wp-content/uploads/2022/02/bradley_spring22.pdf)(2022) é€šè¿‡è±å¸ƒå°¼å…¹å¾®åˆ†æ³•åˆ™(Leibniz rule)å°†ä¿¡æ¯ç†µ,æŠ½è±¡ä»£æ•°,æ‹“æ‰‘å­¦è”ç³»èµ·æ¥ã€‚è¯¥æ–‡ç« æ˜¯ä¸€ä¸ªé›¶åŸºç¡€å¯é˜…è¯»çš„ç»¼è¿°,å…·ä½“å‚è€ƒ[Entropy as a Topological Operad Derivation ](https://www.mdpi.com/1099-4300/23/9/1195)(2021.7,Tai-Danae Bradley.)
24. [minerva](https://storage.googleapis.com/minerva-paper/minerva_paper.pdf)(2022)googleæå‡ºçš„è§£é¢˜æ¨¡å‹,åœ¨å…¬å…±é«˜ç­‰æ•°å­¦ç­‰è€ƒè¯•ä¸­æ¯”äººç±»å¹³å‡åˆ†é«˜.[æµ‹è¯•åœ°å€](https://minerva-demo.github.io/#category=Algebra&index=1).
25. ğŸ”¥[An automatic theorem proving project](https://gowers.wordpress.com/2022/04/28/announcing-an-automatic-theorem-proving-project/#more-6531)è²å°”å…¹è·å¾—è€…æ•°å­¦å®¶é«˜å°”æ–¯å…³äºè‡ªåŠ¨è¯æ˜æ•°å­¦å®šç†çš„é¡¹ç›®è¿›å±•[How can it be feasible to find proofs?](https://drive.google.com/file/d/1-FFa6nMVg18m1zPtoAQrFalwpx2YaGK4/view)(2022, W.T. Gowers).
26. [GRAND: Graph Neural Diffusion ](https://papertalk.org/papertalks/32188)(2021)è¯¥ç½‘ç«™åŒ…å«äº†ä¸€äº›ç›¸ä¼¼è®ºæ–‡èµ„æ–™,[é¡¹ç›®åœ°å€graph-neural-pde](https://github.com/twitter-research/graph-neural-pde),å…¶ä¼˜åŒ–ç‰ˆæœ¬
[GRAND++](https://openreview.net/forum?id=EMxu-dzvJk).(2022).æœ‰åšæ–‡ä»‹ç»[å›¾ç¥ç»ç½‘ç»œçš„å›°å¢ƒï¼Œç”¨å¾®åˆ†å‡ ä½•å’Œä»£æ•°æ‹“æ‰‘è§£å†³](https://mp.weixin.qq.com/s/CFNvgn6vaYcI36QJNa3_dw)ä»…ä¾›å‚è€ƒ.

## DeepModeling
[DeepModeling](https://deepmodeling.com/)é„‚ç»´å—ç­‰ç»„ç»‡,ä¸€ç§æ–°çš„ç ”ç©¶èŒƒå¼,å°†DLå»ºæ¨¡æ¸—é€åˆ°ç§‘ç ”ä¸­,è¿™é‡Œä¼šå¼€æºå¾ˆå¤šå¯¹æ–°æˆ–æ—§é—®é¢˜çš„DLå»ºæ¨¡æ–¹æ¡ˆ.[å…¶githubåœ°å€](https://github.com/deepmodeling).ç©ºäº†çœ‹æƒ…å†µè§£ææŸäº›å·¥ä½œ.
