# Deep Learning Theory
æ•´ç†äº†ä¸€äº›æ·±åº¦å­¦ä¹ çš„ç†è®ºç›¸å…³å†…å®¹ï¼ŒæŒç»­æ›´æ–°ã€‚
## Overview
1. [Recent advances in deep learning theory](https://arxiv.org/pdf/2012.10931.pdf)
æ€»ç»“äº†ç›®å‰æ·±åº¦å­¦ä¹ ç†è®ºç ”ç©¶çš„å…­ä¸ªæ–¹å‘çš„ä¸€äº›ç»“æœï¼Œæ¦‚è¿°å‹ï¼Œæ²¡åšæ·±å…¥æ¢è®¨(2021)ã€‚

    + 1.1 complexity and capacity-basedapproaches for analyzing the generalizability of deep learning; 

    + 1.2 stochastic differential equations andtheir dynamic systems for modelling stochastic gradient descent and its variants, which characterizethe optimization and generalization of deep learning, partially inspired by Bayesian inference; 

    + 1.3 thegeometrical structures of the loss landscape that drives the trajectories of the dynamic systems;

    + 1.4 theroles of over-parameterization of deep neural networks from both positive and negative perspectives; 

    + 1.5 theoretical foundations of several special structures in network architectures; 

    + 1.6 the increasinglyintensive concerns in ethics and security and their relationships with generalizability
2. ğŸ”¥ [On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence](https://arxiv.org/abs/2207.04630)(2022.7.é©¬æ¯…ã€æ²ˆå‘æ´‹ã€æ›¹é¢–)"ä»»ä½•ä¸€ä¸ªæ™ºèƒ½ç³»ç»Ÿï¼Œä½œä¸ºä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„è‡ªä¸»é—­ç¯ï¼šä¿¡æ¯ã€æ§åˆ¶ã€å¯¹ç­–ã€ä¼˜åŒ–ã€ç¥ç»ç½‘ç»œï¼Œç´§å¯†ç»“åˆï¼Œç¼ºä¸€ä¸å¯ã€‚æ€»è€Œè¨€ä¹‹ï¼Œäººå·¥æ™ºèƒ½çš„ç ”ç©¶ä»ç°åœ¨å¼€å§‹ï¼Œåº”è¯¥èƒ½å¤Ÿä¹Ÿå¿…é¡»ä¸ç§‘å­¦ã€æ•°å­¦ã€å’Œè®¡ç®—ç´§å¯†ç»“åˆã€‚ä»æœ€æ ¹æœ¬ã€æœ€åŸºç¡€çš„ç¬¬ä¸€æ€§åŸç†ï¼ˆç®€çº¦ã€è‡ªæ´½ï¼‰å‡ºå‘ï¼ŒæŠŠåŸºäºç»éªŒçš„å½’çº³æ–¹æ³•ä¸åŸºäºåŸºç¡€åŸç†çš„æ¼”ç»æ–¹æ³•ä¸¥æ ¼åœ°ã€ç³»ç»Ÿåœ°ç»“åˆèµ·æ¥å‘å±•ã€‚ç†è®ºä¸å®è·µç´§å¯†ç»“åˆã€ç›¸è¾…ç›¸æˆã€å…±åŒæ¨è¿›æˆ‘ä»¬å¯¹æ™ºèƒ½çš„ç†è§£ã€‚" çŸ¥ä¹è§£è¯»å¯çœ‹ï¼šhttps://zhuanlan.zhihu.com/p/543041107 ã€‚

## Course
2. [Theory of Deep Learning](https://www.ideal.northwestern.edu/special-quarters/fall-2020/)TTIC,è¥¿åŒ—å¤§å­¦ç­‰ç»„ç»‡çš„ä¸€ç³»åˆ—è¯¾ç¨‹å’Œè®²åº§ï¼ŒåŸºç¡€è¯¾ç¨‹æ¶‰åŠDLçš„åŸºç¡€(ç¬¦å·åŒ–ï¼Œç®€åŒ–åçš„æ•°å­¦é—®é¢˜å’Œç»“è®º)ï¼Œä¿¡æ¯è®ºå’Œå­¦ä¹ ï¼Œç»Ÿè®¡å’Œè®¡ç®—ï¼Œä¿¡æ¯è®ºï¼Œç»Ÿè®¡å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ (2020)ã€‚

2+. [2021 Deep learning theory lecture note](https://mjt.cs.illinois.edu/dlt/ )
    + 2.1 é€¼è¿‘ï¼Œä¼˜åŒ–ï¼Œé€šç”¨æ€§ï¼Œä¸‰æ–¹é¢åšäº†æ€»ç»“ï¼Œæ ¸å¿ƒå†…å®¹ç½‘é¡µå¯è§ï¼Œæ¯”è¾ƒå‹å¥½ã€‚

3. [MathsDL-spring19](https://joanbruna.github.io/MathsDL-spring19/),MathDLç³»åˆ—ï¼Œ18,19,20å¹´å‡æœ‰ã€‚

    + 3.1  Geometry of Data

        + Euclidean Geometry: transportation metrics, CNNs , scattering.
        + Non-Euclidean Geometry: Graph Neural Networks.
        + Unsupervised Learning under Geometric Priors (Implicit vs explicit models, microcanonical, transportation metrics).
        + Applications and Open Problems: adversarial examples, graph inference, inverse problems.

    + 3.2 Geometry of Optimization and Generalization

        + Stochastic Optimization (Robbins & Munro, Convergence of SGD)
        + Stochastic Differential Equations (Fokker-Plank, Gradient Flow, Langevin + + Dynamics, links with SGD; open problems)
        Dynamics of Neural Network Optimization (Mean Field Models using Optimal Transport, Kernel Methods)
        + Landscape of Deep Learning Optimization (Tensor/Matrix factorization, Deep Nets; open problems).
        + Generalization in Deep Learning.

    + 3.3  Open qustions on Reinforcement Learning
4. [IFT 6169: Theoretical principles for deep learning](http://mitliagkas.github.io/ift6085-dl-theory-class/)(2022 Winter),å¤§å¤šå†…å®¹è¾ƒä¸ºåŸºç¡€ï¼Œä¼ ç»Ÿã€‚
    + 4.1 æ‹Ÿå®šè¯¾é¢˜
        + Generalization: theoretical analysis and practical bounds
        + Information theory and its applications in ML (information bottleneck, lower bounds etc.)
        +  Generative models beyond the pretty pictures: a tool for traversing the data manifold, projections, completion, substitutions etc.
        + Taming adversarial objectives: Wasserstein GANs, regularization approaches and + controlling the dynamics
        + The expressive power of deep networks (deep information propagation, mean-field analysis of random networks etc.)
5. [æ·±åº¦å­¦ä¹ å‡ ä½•è¯¾ç¨‹](https://geometricdeeplearning.com/lectures/)(2022, Michael Bronstein)å†…å®¹æ¯”è¾ƒé«˜çº§.
    + 5.1 2022 å¹´çš„ GDL100 å…±åŒ…å« 12 èŠ‚å¸¸è§„è¯¾ç¨‹ã€3 èŠ‚è¾…å¯¼è¯¾ç¨‹å’Œ 5 æ¬¡ä¸“é¢˜ç ”è®¨ã€‚12 èŠ‚å¸¸è§„è¯¾ç¨‹ä¸»è¦ä»‹ç»äº†å‡ ä½•æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µçŸ¥è¯†ï¼ŒåŒ…æ‹¬é«˜ç»´å­¦ä¹ ã€å‡ ä½•å…ˆéªŒçŸ¥è¯†ã€å›¾ä¸é›†åˆã€ç½‘æ ¼ï¼ˆgridï¼‰ã€ç¾¤ã€æµ‹åœ°çº¿ï¼ˆgeodesicï¼‰ã€æµå½¢ï¼ˆmanifoldï¼‰ã€è§„èŒƒï¼ˆgaugeï¼‰ç­‰ã€‚3 èŠ‚è¾…å¯¼è¯¾ä¸»è¦é¢å‘è¡¨è¾¾å‹å›¾ç¥ç»ç½‘ç»œã€ç¾¤ç­‰å˜ç¥ç»ç½‘ç»œå’Œå‡ ä½•å›¾ç¥ç»ç½‘ç»œã€‚

    + 5 æ¬¡ä¸“é¢˜ç ”è®¨çš„è¯é¢˜åˆ†åˆ«æ˜¯ï¼š
        1. ä»å¤šç²’å­åŠ¨åŠ›å­¦å’Œæ¢¯åº¦æµçš„è§’åº¦åˆ†æç¥ç»ç½‘ç»œï¼›
        2. è¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„ GNN å­å›¾ï¼›
        3. æœºå™¨å­¦ä¹ ä¸­çš„ç­‰å˜æ€§ï¼›
        4. ç¥ç» sheaf æ‰©æ•£ï¼šä»æ‹“æ‰‘çš„è§’åº¦åˆ†æ GNN ä¸­çš„å¼‚è´¨æ€§å’Œè¿‡åº¦å¹³æ»‘ï¼›
        5. ä½¿ç”¨ AlphaFold è¿›è¡Œé«˜åº¦å‡†ç¡®çš„è›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€‚

6. [Advanced Topics in Machine Learning and Game Theory](https://feifang.info/advanced-topics-in-machine-learning-and-game-theory-fall-2022/)æ¸¸æˆï¼Œå¼ºåŒ–æ–¹é¢çš„è¯¾ç¨‹ï¼Œ2022ã€‚
## Architecture
5. [Partial Differential Equations is All You Need for Generating Neural Architectures -- A Theory for Physical Artificial Intelligence Systems](https://arxiv.org/abs/2103.08313) å°†ç»Ÿè®¡ç‰©ç†çš„ååº”æ‰©æ•£æ–¹ç¨‹ï¼Œé‡å­åŠ›å­¦ä¸­çš„è–›å®šè°”æ–¹ç¨‹ï¼Œå‚è½´å…‰å­¦ä¸­çš„äº¥å§†éœå…¹æ–¹ç¨‹ç»Ÿä¸€æ•´åˆåˆ°ç¥ç»ç½‘ç»œåå¾®åˆ†æ–¹ç¨‹ä¸­(NPDE)ï¼Œåˆ©ç”¨æœ‰é™å…ƒæ–¹æ³•æ‰¾åˆ°æ•°å€¼è§£ï¼Œä»ç¦»æ•£è¿‡ç¨‹ä¸­ï¼Œæ„é€ äº†å¤šå±‚æ„ŸçŸ¥ï¼Œå·ç§¯ç½‘ç»œï¼Œå’Œå¾ªç¯ç½‘ç»œï¼Œå¹¶æä¾›äº†ä¼˜åŒ–æ–¹æ³•L-BFGSç­‰ï¼Œä¸»è¦æ˜¯å»ºç«‹äº†ç»å…¸ç‰©ç†æ¨¡å‹å’Œç»å…¸ç¥ç»ç½‘ç»œçš„è”ç³»(2021)ã€‚

## Approximation
6. NN Approximation Theory
    + 6.0 [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)NNé€¼è¿‘ä»è¿™é‡Œå¼€å§‹(1991)
    + 6.1 [Cybenkoâ€™s Theorem and the capabilityof a neural networkas function approximator](https://www.mathematik.uni-wuerzburg.de/fileadmin/10040900/2019/Seminar__Artificial_Neural_Network__24_9__.pdf)ä¸€äºŒç»´shallowç¥ç»ç½‘ç»œçš„å¯è§†åŒ–è¯æ˜(2019)
    + 6.2 [Depth Separation for Neural Networks](https://arxiv.org/abs/1702.08489) ä¸‰å±‚ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›æ¯”ä¸¤å±‚æœ‰ä¼˜è¶Šæ€§çš„ç®€åŒ–è¯æ˜ (2017)
    + 6.3 [èµµæ‹“ç­‰ä¸‰ç¯‡](https://www.zhihu.com/question/347654789/answer/1480974642)(2019)
    + 6.4 [Neural Networks with Small Weights and Depth-Separation Barriers](https://arxiv.org/abs/2006.00625) Gal Vardiç­‰è¯æ˜äº†å¯¹æŸäº›ç±»å‹çš„ç¥ç»ç½‘ç»œ, ç”¨kå±‚çš„å¤šé¡¹å¼è§„æ¨¡ç½‘ç»œéœ€è¦ä»»æ„weight, ä½†ç”¨3k+3å±‚çš„å¤šé¡¹å¼è§„æ¨¡ç½‘ç»œåªéœ€è¦å¤šé¡¹å¼å¤§å°çš„ weight(2020)ã€‚
    + 6.5 [Universality of Deep Convolutional Neural Networks](https://arxiv.org/pdf/1805.10769.pdf)å·ç§¯ç½‘ç»œçš„é€šç”¨é€¼è¿‘èƒ½åŠ›ï¼ŒåŠå…¶æ ¸å¿ƒè¦ç´ ï¼ŒDing-Xuan Zhou(2018)ï¼Œ20å¹´å‘è¡¨ã€‚

## Optimization
7. SGD
    + 7.1 [nesterov-accelerated-gradient](https://paperswithcode.com/method/nesterov-accelerated-gradient)

8. [offconvex](http://www.offconvex.org/)å‡ ä¸ªå­¦æœ¯å·¥ä½œè€…ç»´æŠ¤çš„AIåšå®¢ã€‚
    + 8.1 [beyondNTK](http://www.offconvex.org/2021/03/25/beyondNTK/) ä»€ä¹ˆæ—¶å€™NNå¼ºäºNTKï¼Ÿ
    + 8.2 [instahide](http://www.offconvex.org/2020/11/11/instahide/) å¦‚ä½•åœ¨ä¸æ³„éœ²æ•°æ®çš„æƒ…å†µä¸‹ä¼˜åŒ–æ¨¡å‹ï¼Ÿ
    + 8.3 [implicit regularization in DL explained by norms?](http://www.offconvex.org/2020/11/27/reg_dl_not_norm/)

9. [Adam](https://arxiv.org/abs/1412.6980)
    + 9.1 [deep-learning-dynamics-paper-list](https://github.com/zeke-xie/deep-learning-dynamics-paper-list)å…³äºDLä¼˜åŒ–åŠ¨åŠ›å­¦æ–¹é¢ç ”ç©¶çš„èµ„æ–™æ”¶é›†ã€‚
    + 9.2 [Adai](https://github.com/zeke-xie/adaptive-inertia-adai) Adamçš„ä¼˜åŒ–ç‰ˆæœ¬Adaiï¼ŒAdamé€ƒç¦»éç‚¹å¾ˆå¿«ï¼Œä½†æ˜¯ä¸èƒ½åƒSGDä¸€æ ·æ“…é•¿æ‰¾åˆ°flat minimaã€‚ä½œè€…è®¾è®¡ä¸€ç±»æ–°çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨Adaiç»“åˆSGDå’ŒAdamçš„ä¼˜ç‚¹ã€‚Adaié€ƒç¦»éç‚¹é€Ÿåº¦æ¥è¿‘Adam,å¯»æ‰¾flat minimaèƒ½æ¥è¿‘SGDã€‚å…¶çŸ¥ä¹ä»‹ç»å¯çœ‹[Adai-zhihu]((https://www.zhihu.com/question/323747423/answer/2576604040))


## Geometry
9. Optima transmission
    + 9.1 [æ·±åº¦å­¦ä¹ çš„å‡ ä½•å­¦è§£é‡Š](http://www.engineering.org.cn/ch/10.1016/j.eng.2019.09.010)(2020)

## Book
10. [Theory of Deep Learning(draft)](https://www.cs.princeton.edu/courses/archive/fall19/cos597B/lecnotes/bookdraft.pdf)Rong Ge ç­‰(2019)ã€‚

11. [Spectral Learning on Matrices and Tensors](https://arxiv.org/pdf/2004.07984.pdf)Majid Janzaminç­‰(2020)

19. [Deep Learning Architectures A Mathematical Approach](https://www.springer.com/gp/book/9783030367206)(2020),ä½ å¯ä»¥libgenè·å–ï¼Œå†…å®¹å¦‚å…¶åå­—,å¤§æ¦‚åŒ…å«ï¼šå·¥ä¸šé—®é¢˜ï¼ŒDLåŸºç¡€(æ¿€æ´»ï¼Œç»“æ„ï¼Œä¼˜åŒ–ç­‰),å‡½æ•°é€¼è¿‘ï¼Œä¸‡æœ‰é€¼è¿‘ï¼ŒRELUç­‰é€¼è¿‘æ–°ç ”ç©¶ï¼Œå‡½æ•°è¡¨ç¤ºï¼Œä»¥åŠä¸¤å¤§æ–¹å‘ï¼Œä¿¡æ¯è§’åº¦ï¼Œå‡ ä½•è§’åº¦ç­‰ç›¸å…³çŸ¥è¯†ï¼Œå®é™…åœºæ™¯ä¸­çš„å·ç§¯ï¼Œæ± åŒ–ï¼Œå¾ªç¯ï¼Œç”Ÿæˆï¼Œéšæœºç½‘ç»œç­‰å…·ä½“å®ç”¨å†…å®¹çš„æ•°å­¦åŒ–ï¼Œå¦å¤–é™„å½•é›†åˆè®ºï¼Œæµ‹åº¦è®ºï¼Œæ¦‚ç‡è®ºï¼Œæ³›å‡½ï¼Œå®åˆ†æç­‰åŸºç¡€çŸ¥è¯†ã€‚
20. [The Principles of Deep Learning Theory](https://arxiv.org/pdf/2106.10165.pdf)(2021)Daniel A. Roberts and Sho Yaida(mit)ï¼ŒBeginning from a first-principles component-level picture of networksï¼Œæœ¬ä¹¦è§£é‡Šäº†å¦‚ä½•é€šè¿‡æ±‚è§£å±‚åˆ°å±‚è¿­ä»£æ–¹ç¨‹å’Œéçº¿æ€§å­¦ä¹ åŠ¨åŠ›å­¦æ¥ç¡®å®šè®­ç»ƒç½‘ç»œè¾“å‡ºçš„å‡†ç¡®æè¿°ã€‚ä¸€ä¸ªä¸»è¦çš„ç»“æœæ˜¯ç½‘ç»œçš„é¢„æµ‹æ˜¯ç”±è¿‘é«˜æ–¯åˆ†å¸ƒæè¿°çš„ï¼Œç½‘ç»œçš„æ·±åº¦ä¸å®½åº¦çš„çºµæ¨ªæ¯”æ§åˆ¶ç€ä¸æ— é™å®½åº¦é«˜æ–¯æè¿°çš„åå·®ã€‚æœ¬ä¹¦è§£é‡Šäº†è¿™äº›æœ‰æ•ˆæ·±åº¦ç½‘ç»œå¦‚ä½•ä»è®­ç»ƒä¸­å­¦ä¹ éå¹³å‡¡çš„è¡¨ç¤ºï¼Œå¹¶æ›´å¹¿æ³›åœ°åˆ†æéçº¿æ€§æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ æœºåˆ¶ã€‚ä»è¿‘å†…æ ¸æ–¹æ³•çš„è§’åº¦æ¥çœ‹ï¼Œå‘ç°è¿™äº›æ¨¡å‹çš„é¢„æµ‹å¯¹åº•å±‚å­¦ä¹ ç®—æ³•çš„ä¾èµ–å¯ä»¥ç”¨ä¸€ç§ç®€å•è€Œé€šç”¨çš„æ–¹å¼æ¥è¡¨è¾¾ã€‚ä¸ºäº†è·å¾—è¿™äº›ç»“æœï¼Œä½œè€…å¼€å‘äº†è¡¨ç¤ºç»„æµï¼ˆRG æµï¼‰çš„æ¦‚å¿µæ¥è¡¨å¾ä¿¡å·é€šè¿‡ç½‘ç»œçš„ä¼ æ’­ã€‚é€šè¿‡å°†ç½‘ç»œè°ƒæ•´åˆ°ä¸´ç•ŒçŠ¶æ€ï¼Œä»–ä»¬ä¸ºæ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜æä¾›äº†ä¸€ä¸ªå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ä½œè€…è¿›ä¸€æ­¥è§£é‡Šäº† RG æµå¦‚ä½•å¯¼è‡´è¿‘ä¹æ™®éçš„è¡Œä¸ºï¼Œä»è€Œå¯ä»¥å°†ç”±ä¸åŒæ¿€æ´»å‡½æ•°æ„å»ºçš„ç½‘ç»œåšç±»åˆ«åˆ’åˆ†ã€‚Altogether, they show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networksã€‚åˆ©ç”¨ä¿¡æ¯ç†è®ºï¼Œä½œè€…ä¼°è®¡äº†æ¨¡å‹æ€§èƒ½æœ€å¥½çš„æœ€ä½³æ·±å®½æ¯”ï¼Œå¹¶è¯æ˜äº†æ®‹å·®è¿æ¥èƒ½å°†æ·±åº¦æ¨å‘ä»»æ„æ·±åº¦ã€‚åˆ©ç”¨ä»¥ä¸Šç†è®ºå·¥å…·ï¼Œå°±å¯ä»¥æ›´åŠ ç»†è‡´çš„ç ”ç©¶æ¶æ„çš„å½’çº³åå·®ï¼Œè¶…å‚æ•°ï¼Œä¼˜åŒ–ã€‚[åŸä½œè€…çš„è§†é¢‘è¯´æ˜](https://www.youtube.com/watch?v=wXZKoHEzASg)(2021.12.1)
21. [Physics-based Deep Learning](https://arxiv.org/pdf/2109.05237.pdf)(2021)N. Thuerey, P. Holl,etc.[github resources](https://github.com/thunil/Physics-Based-Deep-Learning)æ·±åº¦å­¦ä¹ ä¸ç‰©ç†å­¦çš„è”ç³»ã€‚æ¯”å¦‚åŸºäºç‰©ç†çš„æŸå¤±å‡½æ•°ï¼Œå¯å¾®æµä½“æ¨¡æ‹Ÿï¼Œé€†é—®é¢˜çš„æ±‚è§£ï¼ŒNavier-Stokesæ–¹ç¨‹çš„å‰å‘æ¨¡æ‹Ÿï¼ŒControlling Burgersâ€™ Equationå’Œå¼ºåŒ–å­¦ä¹ çš„å…³ç³»ç­‰ã€‚
22. [Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/abs/2104.13478)(Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar VeliÄkoviÄ‡,2021),è§ä¸Šé¢è¯¾ç¨‹5:æ·±åº¦å­¦ä¹ å‡ ä½•è¯¾ç¨‹.

## Session
21. [Foundations of Deep Learning](https://simons.berkeley.edu/programs/dl2019)(2019)ï¼Œè¥¿è’™ç ”ç©¶ä¸­å¿ƒä¼šè®®ã€‚
22. [Deep Learning Theory 4](https://icml.cc/virtual/2021/session/12048)(2021, ICML)Claire Monteleoniä¸»æŒ...,æ·±åº¦å­¦ä¹ ç†è®ºä¼šè®®4ï¼ŒåŒ…å«è®ºæ–‡å’Œè§†é¢‘ã€‚
23. [Deep Learning Theory 5 ](https://icml.cc/virtual/2021/session/12057)(2021,ICML)MaYiä¸»æŒ...ï¼Œæ·±åº¦å­¦ä¹ ç†è®ºä¼šè®®5ï¼ŒåŒ…å«è®ºæ–‡å’Œè§†é¢‘ã€‚

## generalization
1. [Robust Learning with Jacobian Regularization](https://arxiv.org/abs/1908.02729)(2019)Judy Hoffman..., 
2. [Predicting Generalization using GANs](http://www.offconvex.org/2022/06/06/PGDL/)(2022.6),ç”¨GANæ¥è¯„ä¼°æ³›åŒ–æ€§.
3. [Implicit Regularization in Tensor Factorization: Can Tensor Rank Shed Light on Generalization in Deep Learning?](http://www.offconvex.org/2021/07/08/imp-reg-tf/)(2021.7)Tensor Rank èƒ½å¦æ­ç¤ºæ·±åº¦å­¦ä¹ ä¸­çš„æ³›åŒ–ï¼Ÿ
4. [å¦‚ä½•é€šè¿‡Meta Learningå®ç°åŸŸæ³›åŒ–Domain Generalization](https://mp.weixin.qq.com/s/o1liWf9B4_LntBeyV2bVOg)(2022.4),[Domain Generalization CVPR2022](https://mp.weixin.qq.com/s/HkjHEqs8d85VPdgpaHEzPQ)åšæ–‡å‚è€ƒ.
5. [Generalization-Causality](https://github.com/yfzhang114/Generalization-Causality) ä¸€åšå£«å…³äºdomain generalizationç­‰å·¥ä½œçš„å®æ—¶æ±‡æ€»ã€‚
6. [Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Networks](http://www.offconvex.org/2022/07/15/imp-reg-htf-cnn/)(Noam Razin  â€¢  Jul 15, 2022)

    + 6.1 Across three different neural network types (equivalent to matrix, tensor, and hierarchical tensor factorizations), we have an architecture-dependant notion of rank that is implicitly lowered. Moreover, the underlying mechanism for this implicit regularization is identical in all cases. This leads us to believe that implicit regularization towards low rank may be a general phenomenon. If true, finding notions of rank lowered for different architectures can facilitate an understanding of generalization in deep learning.

    + 6.2 Our findings imply that the tendency of modern convolutional networks towards locality may largely be due to implicit regularization, and not an inherent limitation of expressive power as often believed. More broadly, they showcase that deep learning architectures considered suboptimal for certain tasks can be greatly improved through a right choice of explicit regularization. Theoretical understanding of implicit regularization may be key to discovering such regularizers.


## Others
4. [Theoretical issues in deep networks](https://www.pnas.org/content/117/48/30039) è¡¨æ˜æŒ‡æ•°å‹æŸå¤±å‡½æ•°ä¸­å­˜åœ¨éšå¼çš„æ­£åˆ™åŒ–ï¼Œå…¶ä¼˜åŒ–çš„ç»“æœå’Œä¸€èˆ¬æŸå¤±å‡½æ•°ä¼˜åŒ–ç»“æœä¸€è‡´ï¼Œä¼˜åŒ–æ”¶æ•›ç»“æœå’Œæ¢¯åº¦æµçš„è¿¹æœ‰å…³ï¼Œç›®å‰è¿˜ä¸èƒ½è¯æ˜å“ªä¸ªç»“æœæœ€ä¼˜(2020)ã€‚
12. [The Dawning of a New Erain Applied Mathematics](https://www.ams.org/journals/notices/202104/rnoti-p565.pdf)Weinan Eå…³äºåœ¨DLçš„æ–°å¤„å¢ƒä¸‹ç»“åˆå†å²çš„å·¥ä½œèŒƒå¼ç»™å‡ºçš„æŒ‡å¯¼æ€§æ€»ç»“(2021)ã€‚
13. [Mathematics of deep learning from Newton Institute](https://www.newton.ac.uk/event/mdl)ã€‚
14. [DEEP NETWORKS FROM THE PRINCIPLE OF RATE REDUCTION](https://openreview.net/forum?id=G70Z8ds32C9)ï¼Œç™½ç›’ç¥ç»ç½‘ç»œã€‚
15. [redunet_paper](https://github.com/ryanchankh/redunet_paper)ç™½ç›’ç¥ç»ç½‘ç»œä»£ç ã€‚
16. [Theory of Deep Convolutional Neural Networks:Downsampling](https://www.cityu.edu.hk/rcms/pdf/XDZhou/dxZhou2020b.pdf)ä¸‹é‡‡æ ·çš„æ•°å­¦åˆ†æDing-Xuan Zhou(2020)
17. [Theory of deep convolutional neural networks II: Spherical analysis](https://arxiv.org/abs/2007.14285)è¿˜æœ‰IIIï¼šradial functions é€¼è¿‘ï¼Œ(2020)ã€‚ä¸è¿‡è¿™äº›å·¥ä½œåˆ°åº•å¦‚ä½•ï¼Œåªæ˜¯ç”¨æ•°å­¦è½¬æ¢äº†ä¸€ä¸‹ï¼Œç†è®ºä¸Šæ²¡åšè¿‡å¤šè´¡çŒ®ï¼Œæˆ–è€…å’Œå®é™…ç»“åˆæ²¡éš¾ä¹ˆç´§å¯†ï¼Œè¿˜ä¸å¾—è€ŒçŸ¥ã€‚
18. [The Modern Mathematics of Deep Learning](https://arxiv.org/abs/2105.04026)(2021)ä¸»è¦æ˜¯deep laerningçš„æ•°å­¦åˆ†ææè¿°ï¼Œæ¶‰åŠçš„é—®é¢˜åŒ…æ‹¬ï¼šè¶…å‚æ•°ç½‘ç»œçš„é€šç”¨èƒ½åŠ›ï¼Œæ·±åº¦åœ¨æ·±åº¦æ¨¡å‹ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ·±åº¦å­¦ä¹ å¯¹ç»´åº¦ç¾éš¾çš„å…‹æœï¼Œä¼˜åŒ–åœ¨éå‡¸ä¼˜åŒ–é—®é¢˜çš„æˆåŠŸï¼Œå­¦ä¹ çš„è¡¨ç¤ºç‰¹å¾çš„æ•°å­¦åˆ†æï¼Œä¸ºä½•æ·±åº¦æ¨¡å‹åœ¨ç‰©ç†é—®é¢˜ä¸Šæœ‰è¶…å¸¸è¡¨ç°ï¼Œæ¨¡å‹æ¶æ„ä¸­çš„å“ªäº›å› ç´ ä»¥ä½•ç§æ–¹å¼å½±å“ä¸åŒä»»åŠ¡çš„å­¦ä¹ ä¸­çš„ä¸åŒæ–¹é¢ã€‚
19. [Topos and Stacks of Deep Neural Networks](https://arxiv.org/abs/2106.14587)(2021)æ¯ä¸€ä¸ªå·²çŸ¥çš„æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)å¯¹åº”äºä¸€ä¸ªå…¸å‹çš„ Grothendieck çš„ topos ä¸­çš„ä¸€ä¸ªå¯¹è±¡; å®ƒçš„å­¦ä¹ åŠ¨æ€å¯¹åº”äºè¿™ä¸ª topos ä¸­çš„ä¸€ä¸ªæ€å°„æµã€‚å±‚ä¸­çš„ä¸å˜æ€§ç»“æ„(å¦‚ CNNs æˆ– LSTMs)ä¸Giraud's stacksç›¸å¯¹åº”ã€‚è¿™ç§ä¸å˜æ€§è¢«è®¤ä¸ºæ˜¯æ³›åŒ–æ€§è´¨çš„åŸå› ï¼Œå³ä»çº¦æŸæ¡ä»¶ä¸‹çš„å­¦ä¹ æ•°æ®è¿›è¡Œæ¨æ–­ã€‚çº¤ç»´ä»£è¡¨å‰è¯­ä¹‰ç±»åˆ«(Culioliï¼ŒThom) ï¼Œå…¶ä¸Šäººå·¥è¯­è¨€çš„å®šä¹‰ï¼Œå†…éƒ¨é€»è¾‘ï¼Œç›´è§‰ä¸»ä¹‰ï¼Œç»å…¸æˆ–çº¿æ€§(Girard)ã€‚ç½‘ç»œçš„è¯­ä¹‰åŠŸèƒ½æ˜¯ç”¨è¿™ç§è¯­è¨€è¡¨è¾¾ç†è®ºçš„èƒ½åŠ›ï¼Œç”¨äºå›ç­”è¾“å…¥æ•°æ®è¾“å‡ºä¸­çš„é—®é¢˜ã€‚è¯­ä¹‰ä¿¡æ¯çš„é‡å’Œç©ºé—´çš„å®šä¹‰ä¸é¦™å†œç†µçš„åŒæºè§£é‡Šç›¸ç±»ä¼¼ã€‚ä»–ä»¬æ¨å¹¿äº† Carnap å’Œ Bar-Hillel (1952)æ‰€å‘ç°çš„åº¦é‡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸Šè¿°è¯­ä¹‰ç»“æ„è¢«åˆ†ç±»ä¸ºå‡ ä½•çº¤ç»´å¯¹è±¡åœ¨ä¸€ä¸ªå°é—­çš„Quillenæ¨¡å‹èŒƒç•´ï¼Œç„¶åä»–ä»¬å¼•èµ·åŒæ—¶å±€éƒ¨ä¸å˜çš„ dnn å’Œä»–ä»¬çš„è¯­ä¹‰åŠŸèƒ½ã€‚Intentional type theories (Martin-Loef)ç»„ç»‡è¿™äº›å¯¹è±¡å’Œå®ƒä»¬ä¹‹é—´çš„çº¤ç»´åŒ–ã€‚ä¿¡æ¯å†…å®¹å’Œäº¤æ¢ç”± Grothendieck's derivatorsåˆ†æã€‚
20. [Visualizing the Emergence of Intermediate Visual Patterns in DNNs](https://arxiv.org/abs/2111.03505)(2021,NIPS)æ–‡ç« è®¾è®¡äº†ä¸€ç§ç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„å¯è§†åŒ–æ–¹æ³•ï¼Œä½¿å¾—èƒ½
ï¼ˆ1ï¼‰æ›´ç›´è§‚åœ°åˆ†æç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶ä¸”å±•ç¤ºä¸­å±‚ç‰¹å¾è¡¨è¾¾èƒ½åŠ›çš„æ—¶ç©ºæ¶Œç°ï¼›
ï¼ˆ2ï¼‰é‡åŒ–ç¥ç»ç½‘ç»œä¸­å±‚çŸ¥è¯†ç‚¹ï¼Œä»è€Œå®šé‡åœ°åˆ†æç¥ç»ç½‘ç»œä¸­å±‚ç‰¹å¾çš„è´¨é‡ï¼›
ï¼ˆ3ï¼‰ä¸ºä¸€äº›æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚å¯¹æŠ—æ”»å‡»ã€çŸ¥è¯†è’¸é¦ï¼‰æä¾›æ–°è§è§£ã€‚
21.  [ç¥ç»ç½‘ç»œçš„åšå¼ˆäº¤äº’è§£é‡Šæ€§](https://zhuanlan.zhihu.com/p/264871522/)(çŸ¥ä¹)ã€‚ä¸Šäº¤å¤§å¼ æ‹³çŸ³å›¢é˜Ÿç ”ç©¶è®ºæ–‡æ•´ç†è€Œå¾—ï¼Œä½œä¸ºåšå¼ˆäº¤äº’è§£é‡Šæ€§çš„ä½“ç³»æ¡†æ¶ï¼ˆä¸æ€ä¹ˆç¨³å›ºï¼‰ã€‚
22.  [Advancing mathematics by guiding human intuition with AI](https://www.nature.com/articles/s41586-021-04086-x)(2021,nature)æœºå™¨å­¦ä¹ å’Œæ•°å­¦å®¶å·¥ä½œçš„ä¸€ä¸ªæœ‰æœºç»“åˆï¼Œä¸»è¦åˆ©ç”¨æœºå™¨å­¦ä¹ åˆ†æä¼—å¤šç‰¹å¾å’Œç›®æ ‡å˜é‡çš„ä¸»è¦ç›¸å…³å› å­ï¼ŒåŠ å¼ºæ•°å­¦å®¶çš„ç›´è§‰ï¼Œè¯¥è®ºæ–‡å¾—åˆ°äº†ä¸¤ä¸ªæ¼‚äº®çš„å®šç†ï¼Œä¸€ä¸ªæ‹“æ‰‘ï¼Œä¸€ä¸ªè¡¨ç¤ºè®ºã€‚å¯å‚è€ƒ[å›ç­”](https://www.zhihu.com/question/503185412/answer/2256015652)ã€‚
23. ğŸ”¥[A New Perspective of Entropy](https://math3ma.institute/wp-content/uploads/2022/02/bradley_spring22.pdf)(2022) é€šè¿‡è±å¸ƒå°¼å…¹å¾®åˆ†æ³•åˆ™(Leibniz rule)å°†ä¿¡æ¯ç†µ,æŠ½è±¡ä»£æ•°,æ‹“
æ‰‘å­¦è”ç³»èµ·æ¥ã€‚è¯¥æ–‡ç« æ˜¯ä¸€ä¸ªé›¶åŸºç¡€å¯é˜…è¯»çš„ç»¼è¿°,å…·ä½“å‚è€ƒ[Entropy as a Topological Operad Derivation ](https://www.mdpi.com/1099-4300/23/9/1195)(2021.7,Tai-Danae Bradley.)
24. [minerva](https://storage.googleapis.com/minerva-paper/minerva_paper.pdf)(2022)googleæå‡ºçš„è§£é¢˜æ¨¡å‹,åœ¨å…¬å…±é«˜ç­‰æ•°å­¦ç­‰è€ƒè¯•ä¸­æ¯”äººç±»å¹³å‡åˆ†é«˜.[æµ‹è¯•åœ°å€](https://minerva-demo.
github.io/#category=Algebra&index=1).
25. ğŸ”¥[An automatic theorem proving project](https://gowers.wordpress.com/2022/04/28/announcing-an-automatic-theorem-proving-project/#more-6531)è²å°”å…¹è·å¾—è€…æ•°å­¦å®¶é«˜å°”æ–¯å…³äº
è‡ªåŠ¨è¯æ˜æ•°å­¦å®šç†çš„é¡¹ç›®è¿›å±•[How can it be feasible to find proofs?](https://drive.google.com/file/d/1-FFa6nMVg18m1zPtoAQrFalwpx2YaGK4/view)(2022, W.T. Gowers).
26. [GRAND: Graph Neural Diffusion ](https://papertalk.org/papertalks/32188)(2021)è¯¥ç½‘ç«™åŒ…å«äº†ä¸€äº›ç›¸ä¼¼è®ºæ–‡èµ„æ–™,[é¡¹ç›®åœ°å€graph-neural-pde](https://github.com/twitter-research
/graph-neural-pde),å…¶ä¼˜åŒ–ç‰ˆæœ¬[GRAND++](https://openreview.net/forum?id=EMxu-dzvJk).(2022).æœ‰åšæ–‡ä»‹ç»[å›¾ç¥ç»ç½‘ç»œçš„å›°å¢ƒï¼Œç”¨å¾®åˆ†å‡ ä½•å’Œä»£æ•°æ‹“æ‰‘è§£å†³](https://mp.weixin.qq.com/s/CFNvgn6vaYcI36QJNa3_dw)ä»…ä¾›å‚
è€ƒ.
27. [Weinan Ãˆ-A Mathematical Perspective on Machine Learning](https://opade.digital/)(2022.icm),room1æœ€åä¸€æ’,é„‚ç»´å—åœ¨icmçš„æ¼”è®²è§†é¢‘.
28.  [contrastive learning](https://zhuanlan.zhihu.com/p/524733769)è¯æ˜åŒ…æ‹¬InfoNCEåœ¨å†…çš„ä¸€å¤§ç±»å¯¹æ¯”å­¦ä¹ ç›®æ ‡å‡½æ•°ï¼Œç­‰ä»·äºä¸€ä¸ªæœ‰ä¸¤ç±»å˜é‡ï¼ˆæˆ–è€…è¯´ä¸¤ç±»ç©å®¶ï¼‰å‚ä¸çš„äº¤æ›¿ä¼˜åŒ–ï¼ˆæˆ–è€…è¯´æ¸¸æˆï¼‰è¿‡ç¨‹.
<<<<<<< HEAD
29.  [å¯è§£é‡Šæ€§ï¼šBatch Normalizationæœªå¿…å®¢è§‚åœ°ååº”æŸå¤±å‡½æ•°ä¿¡æ¯](https://zhuanlan.zhihu.com/p/523627298)2022,å¼ æ‹³çŸ³ç­‰.
30. [Homotopy Theoretic and Categorical Models of Neural Information Networks](https://arxiv.org/abs/2006.15136)è¯¥å·¥ä½œç¬¬ä¸€ä½œè€…ä¿„ç½—æ–¯æ•°å­¦å®¶Yuri Maninï¼Œ2020å·¥ä½œï¼Œ2022å¹´8æœˆarxivæœ‰æ›´æ–°ã€‚[ncatlabæœ‰è®¨è®º](https://nforum.ncatlab.org/discussion/13133/understanding-preprint-topos-and-stacks-of-deep-neural-networks/)ã€‚
31. [Deep learning via dynamical systems: An approximation perspective](https://ems.press/journals/jems/articles/5404458)åŠ¨åŠ›ç³»ç»Ÿé€¼è¿‘ã€‚[è®ºæ–‡è§](https://cpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/d/11132/files/2021/01/main-jems.pdf)ã€‚
32. [ç¾¤è®ºè§’åº¦](https://www.youtube.com/playlist?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd)ç¾¤è®ºè§’åº¦å»ç†è§£çš„ä¸€ç³»åˆ—è§†é¢‘ï¼Œç¾¤è®ºè§†è§’ï¼Œ2014å¹´å‡ºç°è¿‡ï¼Œè§†é¢‘ç³»ç»Ÿè®²è§£ï¼Œ2022å¹´ã€‚
=======
29.  [å¯è§£é‡Šæ€§ï¼šBatch Normalizationæœªå¿…å®¢è§‚åœ°ååº”æŸå¤±å‡½æ•°ä¿¡æ¯](https://zhuanlan.zhihu.com/p/523627298)2022,å¼ æ‹³çŸ³ç­‰.BNæ“ä½œä½¿å¾—ç¥ç»ç½‘ç»œè®­ç»ƒæ— æ³•å®¢è§‚ååº”æŸå¤±å‡½æ•°çš„ä¿¡æ¯ã€‚å…·ä½“åœ°ï¼Œæœ¬æ–‡ç†è®ºè¯æ˜äº†å½“æˆ‘ä»¬å¯¹æ·±åº¦ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°è¿›è¡Œæ³°å‹’å±•å¼€æ—¶ï¼Œ BNæ“ä½œé˜»å¡äº†æŸå¤±å‡½æ•°çš„ä¸€é˜¶é¡¹å’ŒäºŒé˜¶é¡¹çš„åå‘ä¼ æ’­ã€‚åœ¨å®éªŒä¸­ï¼Œä½œè€…å‘ç°BNæ“ä½œæœ‰æ—¶å€™é˜»ç¢äº†ç¥ç»ç½‘ç»œå¯¹ç‰¹å®šç‰¹å¾çš„å­¦ä¹ ã€‚è®ºæ–‡çš„ç»“è®ºæ˜¯å¾ˆæ¸…æ™°çš„,å…·ä½“çœ‹è®ºæ–‡.
30.  [Why Adversarial Training of ReLU Networks is Difficult?](https://arxiv.org/abs/2205.15130)2022.5,Xu Chengç­‰
     1. æˆ‘ä»¬æ¨å¯¼äº†å¯¹æŠ—æ‰°åŠ¨çš„è§£æè§£ï¼Œè¯¥è§£æè§£æ­ç¤ºäº†å¯¹æŠ—æ‰°åŠ¨çš„åŠ¨åŠ›å­¦æœ¬è´¨ã€‚
     2. åŸºäºå¯¹æŠ—æ‰°åŠ¨çš„è§£æè§£ï¼Œæˆ‘ä»¬ç†è®ºè§£é‡Šäº†å¯¹æŠ—è®­ç»ƒä¼˜åŒ–å›°éš¾çš„åŸå› ã€‚
     3. åŸºäºä¸Šè¿°ç†è®ºï¼Œæˆ‘ä»¬ç»Ÿä¸€åˆ†æåç¯‡å‰äººå¯¹å¯¹æŠ—è®­ç»ƒçš„ç ”ç©¶çš„å†…åœ¨æœºç†ã€‚
     4. å¯¹æŠ—æ‰°åŠ¨ä¼šå¢å¼ºè¾“å…¥xçš„HessiançŸ©é˜µæœ€å¤§ç‰¹å¾å€¼å¯¹åº”ç‰¹å¾å‘é‡æ–¹å‘ä¸Šçš„æ¢¯åº¦åˆ†é‡ï¼Œå¹¶ä¸”å½“å¯¹æŠ—åŠ›åº¦è¾ƒå¤§æ—¶ï¼Œå¯¹æŠ—æ‰°åŠ¨åªä¼šæ²¿ç€å°‘æ•°ç‰¹å¾å€¼è¾ƒå¤§çš„æ–¹å‘å˜åŒ–ã€‚
     5. å¯¹æŠ—è®­ç»ƒå¯ä»¥è¢«è®¤ä¸ºå¢åŠ äº†HessiançŸ©é˜µå¯¹ç½‘ç»œå‚æ•°æ›´æ–°çš„å½±å“ï¼Œè¿™ä½¿å¾—å¯¹æŠ—è®­ç»ƒç›¸è¾ƒäºæ­£å¸¸è®­ç»ƒï¼Œæ›´å®¹æ˜“å‡ºç°ç½‘ç»œå‚æ•°åœ¨æŸä¸€æ–¹å‘æŒ¯è¡çš„æƒ…å†µï¼Œå¢åŠ äº†å¯¹æŠ—è®­ç»ƒçš„å›°éš¾ç¨‹åº¦ã€‚
     6. ç­‰
31. [Trap of Feature Diversity in the Learning of MLPs](https://arxiv.org/abs/2112.00980)2022.7,Dongrui Liuç­‰.
    1.  æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé•¿æœŸè¢«å¿½ç•¥çš„åŸºç¡€ä½†åç›´è§‰çš„ç°è±¡ï¼Œå³åœ¨å¤šå±‚æ„ŸçŸ¥æœºè®­ç»ƒçš„å‰æœŸï¼ˆåœ¨å¾ˆæ—©æœŸLossä¸ä¸‹é™çš„é‚£ä¸€æ®µï¼‰ï¼Œç‰¹å¾å¤šæ ·æ€§ä¼šå¿«é€Ÿä¸‹é™ï¼Œç”šè‡³ä¸åŒç±»åˆ«çš„ä¸åŒæ ·æœ¬å­¦ä¹ åˆ°å‡ ä¹ä¸€æ ·çš„ç‰¹å¾ã€‚è¿™æ—¶ç¥ç»ç½‘ç»œæœ‰äº›åƒä¸€ä¸ªè‡ªæ¿€ç³»ç»Ÿã€‚è¿™ç§ç°è±¡æ˜¯ååˆ†åå¸¸çš„ï¼Œè€Œä¸”å®ƒä¼šç ´åå¤šå±‚æ„ŸçŸ¥æœºçš„è®­ç»ƒã€‚
    2.  å½“è®­ç»ƒä»»åŠ¡æ¯”è¾ƒéš¾çš„æ—¶å€™ï¼Œå¾ˆå¤šç¥ç»ç½‘ç»œå¾€å¾€â€œè®­ç»ƒä¸åŠ¨â€ï¼Œç†è®ºä¸Šæ°æ°å±äºå¡åœ¨äº†è¿™ä¸ªç¥ç»ç½‘ç»œçš„ç³»ç»Ÿè‡ªæ¿€é˜¶æ®µã€‚
    3.  æˆ‘ä»¬ä»å­¦ä¹ åŠ¨åŠ›å­¦ (learning dynamics) çš„è§’åº¦ç†è®ºä¸Šè§£é‡Šäº†è¿™ä¸€ç°è±¡ï¼Œå¹¶ä¸”åŸºäºç†è®ºåˆ†æï¼Œæˆ‘ä»¬è§£é‡Šäº†å››ç§å¯ä»¥ç¼“è§£è¿™ä¸ªç°è±¡æ“ä½œçš„åŸå› ã€‚
    4.  [åŸä½œè€…è§£æ](https://zhuanlan.zhihu.com/p/521453526)



>>>>>>> dcbd248fa1d60b14e14dbff7edc6f0ec963f48c8
 ## DeepModeling
 [DeepModeling](https://deepmodeling.com/)é„‚ç»´å—ç­‰ç»„ç»‡,ä¸€ç§æ–°çš„ç ”ç©¶èŒƒå¼,å°†DLå»ºæ¨¡æ¸—é€åˆ°ç§‘ç ”ä¸­,è¿™é‡Œä¼šå¼€æºå¾ˆå¤šå¯¹æ–°æˆ–æ—§é—®é¢˜çš„DLå»ºæ¨¡æ–¹æ¡ˆ.[å…¶githubåœ°å€](https://github.com/deepmode
ling).ç©ºäº†çœ‹æƒ…å†µè§£ææŸäº›å·¥ä½œ.
 
 ## æ•°å­¦å½¢å¼ä¸»ä¹‰ä¸è®¡ç®—æœº
1. [The Future of Mathematicsï¼Ÿ ](https://www.bilibili.com/video/av71583469)(2019) Kevin Buzzardå°±leançš„ä¸€åœºè®²åº§ï¼Œè¯„è®ºåŒºæœ‰å¯¹åº”è®²ä¹‰èµ„æ–™ã€‚
2. [æ•°å­¦å½¢å¼ä¸»ä¹‰çš„å…´èµ·](https://mp.weixin.qq.com/s/-XosE3LzA8wfFv-38EIfKQ)(2022.7)Kevin Buzzardæ•™æˆåœ¨2022æœ¬å±Šå›½é™…æ•°å­¦å®¶å¤§ä¼šä¸€å°æ—¶æŠ¥å‘Šæ¼”è®²ä¸­æä¾›äº†ä¸€äº›ä¿¡æ¯å’Œæ€è€ƒè§è§£ã€‚è®²è¿°äº†æ•°å­¦
å½¢å¼ä¸»ä¹‰ä¸äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œå¼€æºç¤¾åŒºçš„å…±åŒåŠªåŠ›ï¼Œç”¨è®¡ç®—æœºåšå¥¥æ•°é¢˜ã€æ£€æŸ¥æ•°å­¦è¯æ˜è¿‡ç¨‹æ˜¯å¦æœ‰è¯¯ã€ç”šè‡³è‡ªåŠ¨å‘ç°å’Œå½¢å¼åŒ–è¯æ˜æ•°å­¦å®šç†ï¼Œåœ¨ç†è®ºå’Œå®è·µä¸­åˆä¼šç¢°æ’å‡ºä»€ä¹ˆç«èŠ±ï¼Œåˆä¼šå¦‚ä½•å›¿äº...
3. [ä¸“è®¿ICM 2022å›½é™…æ•°å­¦å®¶å¤§ä¼šä¸€å°æ—¶æŠ¥å‘Šè€…Kevin Buzzardï¼šè®¡ç®—æœºå¯ä»¥æˆä¸ºæ•°å­¦å®¶å—ï¼Ÿâ€”â€”è¯‘è‡ªé‡å­æ‚å¿—](https://mp.weixin.qq.com/s/VWuRyxkl0xgZWcqRn0WJAw)æ¯”è¾ƒå¥½çš„é‡‡è®¿,å€¼å¾—çœ‹çœ‹.æ•°å­¦å®¶è®©è®¡ç®—æœºç§‘å­¦å®¶äº†è§£åˆ°æ•°å­¦å¾ˆéš¾,è¿™ä¸ªéƒ¨åˆ†,åœ¨è¢«é€æ¸ç†è§£,ä¸”è®¡ç®—æœºç³»ç»Ÿæ£€æŸ¥,å¯èƒ½ä¼šè§£å†³è¿™ä¸ªéš¾ç‚¹.è¿˜æœ‰é‚£äº›ç‚«é…·çš„é¡¹ç›®,çƒé¢å¤–ç¿»,è´¹é©¬å¤§å®šç†,éå¸¸å€¼å¾—å…³æ³¨.
4. [Deep Maths-machine learning and mathematics](https://www.youtube.com/watch?v=wbJQTtjlM_w),é‡æ–°å‘ç°Eulerå¤šé¢ä½“å…¬å¼ ï¼ˆå¯¹ä¹‹å‰å·¥ä½œçš„ç»†èŠ‚çš„æ›´è¿›ä¸€æ­¥è¯´æ˜ï¼‰,æ¶‰åŠç»„åˆä¸å˜é‡çŒœæƒ³ï¼ŒåºåŠ è±çŒœæƒ³ï¼Œç‘Ÿæ–¯é¡¿å‡ ä½•çŒœæƒ³ï¼Œæ‰­ç»“å›¾ç­‰ï¼ˆæ¶‰åŠçš„é¢å¾ˆå¤§ï¼Œä½†éƒ½æ˜¯ä¸€å¸¦è€Œè¿‡ï¼‰ã€‚![image](./imgs/ex1_knot.png)
## Discussion
1. [æ€æ ·çœ‹å¾…Ali Rahimi è·å¾— NIPS 2017 Test-of-time Awardåçš„æ¼”è®²ï¼Ÿ](https://www.zhihu.com/question/263711574)17å¹´å°±æœ‰äºº(å¼ å¿ƒæ¬£,ç‹åˆšç­‰)æŒ‡å‡ºäº†DLçš„ç¼ºé™·,å’Œè¿™ä¸ªé¢†åŸŸä¸­äººçš„ç‰¹ç‚¹,è¿‡å»5å¹´äº†,è¿˜æ˜¯é‚£æ ·.ä¸è¿‡å¦‚23 èƒ½çœ‹å‡º,metaçš„åšåº”ç”¨çš„ç”°æ¸Šæ ‹è¿˜åœ¨åšå®ˆç†è®º.
2. [æ·±åº¦å­¦ä¹ é¢†åŸŸæœ‰å“ªäº›ç“¶é¢ˆï¼Ÿ](https://www.zhihu.com/question/40577663/answer/2593884415)å¼ æ‹³çŸ³æ–°çš„åæ§½,ä»¥åŠæœ€æ–°æˆæœæ±‡é›†.
